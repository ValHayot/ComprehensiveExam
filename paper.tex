\documentclass{report}                                                           
        
\usepackage{titlesec}
\titleformat{\chapter}
  {\normalfont\LARGE\bfseries}{\thechapter}{1em}{}
\titlespacing*{\chapter}{0pt}{3.5ex plus 1ex minus .2ex}{2.3ex plus .2ex}
\usepackage[left=2cm, right=2cm, top=2cm, bottom=2cm]{geometry}
\usepackage{hyperref} 
\usepackage{xcolor} 
\usepackage{ulem}                      
                                                                                 
\newcommand{\note}[1]{\textcolor{blue}{\textit{note}: #1}}                       
\newcommand{\tristan}[1]{\textcolor{red}{TG: #1}}                                
\newcommand{\weird}[1]{\uwave{#1}}  


\begin{document} 
    \title{Pipeline systems and infrastructure for the efficient
            and open processing of Big neuroimaging Data} 
    \author{Valerie Hayot-Sasson}
    \maketitle 
    
    \begin{abstract} Text of abstract.  
    \end{abstract} 
    \tableofcontents
    \chapter{Introduction}
    In recent years, the volume of neuroimaging data acquired has exceeded
    both the storage and computation capacity of a standard research 
    lab workstation. With the advancement of data sharing technologies, this 
    data has been made widely available, with the only limiting factor impeding
    research on this data being access to infrastructure and software that 
    enables efficient processing on such infrastructure. Many research labs do
    have access to high performance computing (HPC) clusters, however, without
    efficient software to process this data, processing can take an excessive 
    amount of time. Ensuring efficient processing of data is complex and 
    likely beyond the knowledge of an average neuroimaging researcher. As a 
    result, frameworks for the efficient processing of neuroimaging data need
    to be developed.

    Neuroimaging big data comes in two formats: 1) large images and 2) large 
    datasets. Large images consist of a singular images ranging from 100s of
    gigabytes to terabytes in size. A well-known example of such an image is the
    BigBrain \note{cite here}, a histological image of the brain of a 
    healthy 69 year-old man. 
    At its highest resolution, it is 1x1x20micrometers \note{fix this} or 1TB in
    size. Other examples of large images can be found in electron microscopy 
    (EM), polarized light images (PLI) and micro coherence tomography (microCT)
    \note{need to do more research here}. Images at such high resolution are
    important to researchers as they give insights into aspects not otherwise
    detectible in images at lower resolutions. However, due to their size and 
    lack of resources available to process such images, research on this data 
    remains limited.

    In contrast, large datasets are consists of many images, typically arising
    from multiple different subjects, that are too big to fit in storage. 
    The images alone being small enough for an average researcher to process. 
    Example of these datasets include the UK Biobank, HCP Project, ADNI dataset,
    OpenfMRI, amongst many others. These datasets are very large. For instance, 
    the UK Biobank is expected to exceed \note{X} Petabytes in size. As subsets
    of these datasets are manageable for the average researcher to process, 
    much analysis has gone into them. Moreover, unlike for large images, 
    toolkits have been developed for researchers to process this data. However,
    the toolkits do not enable researchers to process these entire datasets as
    a whole. Therefore, it is crucial to develop efficient frameworks for the 
    processing of these datasets in order for researchers to harness the data
    in its entirety.

        \section{Platforms for  neuroscience}
        \section{Infrastructure}


    \chapter{Workflow composition}
    \chapter{Performance}
    \chapter{Reproducibility}
    \chapter{Portability}
    \addcontentsline{toc}
        {chapter}{Bibliography} 
        \bibliography{bibliography}
        \bibliographystyle{ieeetr}
\end{document}
