\documentclass{report}

\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{ulem}

\newcommand{\note}[1]{\textcolor{green}{\textit{note}: #1}}
\newcommand{\tristan}[1]{\textcolor{red}{TG: #1}}
\newcommand{\weird}[1]{\uwave{#1}}

\begin{document}
\title{Pipeline systems and infrastructure for the efficient and open processing of Big neuroimaging Data}
\author{Valerie Hayot-Sasson}
\maketitle
\begin{abstract}
  Text of abstract.  
\end{abstract}
\tableofcontents
\chapter{Introduction}
intro here -mention hpc, data centre, cloud. 
\chapter{BigData and Open science in neuroimaging}
    \note{Likely will not be split in two separate sections}
    \section{large images}
        \begin{itemize}  
            \item BigBrain
            \item Micro CT 
            \item EM 
        \end{itemize}
    \section{large datasets}
    	\note{off the top of my head, but need to double check}
        \begin{itemize}
            \item HCP
            \item ADNI
            \item OpenFMRI
            \item BIDS/BIDS apps
        \end{itemize}
    An effective engine to process such data requires the following characteristics
        \begin{itemize}
            \item in-memory computing
            \item data locality
            \item lazy evaluation
        \end{itemize}
        \section{open-science /data sharing platforms for neuroimaging}
        		is open science just data sharing, computing. 
        		\subsection{Introduction}
		\subsection{CBRAIN}
		\subsection{OpenNEURO}
		\subsection{NeuroData}
\chapter{Workflow engines for neuroimaging data}
	\section{Definition of workflows/pipelines}

    Broad definition of activities - between the tool and the engine.
	\section{Nipype}
		\subsection{summary}
		Nipype\cite{nipype} is a popular neuroimaging pipelinening framework written in Python.
		It provides users with uniform access to the rich ecosystem of neuroimaging software
		libraries (e.g. SPM, FSL, Freesurfer) through its \textit{Interfaces}. \weird{Should users want to
		use their own custom tool, it is also possible to create an interface for that}. To execute 
		Python code in Nipype, the \textit{Function} interface can be used \tristan{for arbitrary code?}. The framework is 
		also easy to use, and thus does not contribute to the steep learning curve new
		researchers must face.
		
		A pipeline in Nipype, consists of a data analysis. explicit
                \textit{Workflow} of connected
                \textit{interfaces}. The workflow is a directed
                acyclic graph that connects the outputs of one
                interface to be the inputs of another. A workflow may
                also consist of connected workflows.

                \tristan{The text below is about workflow execution
                  while the text above is about workflow description,
                  this should be a new paragraph.}  Workflows may be
                executed in parallel either locally or an a cluster
                through the use of a plugin in the Workflow's run
                function. For debugging, Nipype uses \textit{graphviz}
                to generate a static graph representing the Nodes and
                their relationships. \tristan{debugging might be a
                  third theme, in addition to description and
                  execution. Provenance might go in debugging.}
		
		 Each \textit{interface} within the workflow must be
                 contained within a \textit{Node} or a
                 \textit{MapNode} object, the MapNode class being a
                 subclass of the Node class. Wrapping an interface in
                 such objects ensures that interfaces are executed
                 within a uniquely named directories, which in turn
                 enables provenance tracking. Node objects also hash
                 inputs, provide the ability to iterate over inputs
                 and cache results. Hash inputs are useful in the case
                 of Workflow recomputation, where Nipype will only
                 recompute Nodes whose inputs have changed since the
                 previous run.
		
		Inspired by the MapReduce paradigm, the MapNode copies
                the interface to each input and executes it
                independently. \tristan{How are inputs defined/combined?} A reduce is subsequently performed by
                the MapNode to return the output in the form of a
                list. This differs from a Node object which can only
                execute the interface on a single input at a time.
		
		To enable execution of different parameters on the
                input, Nodes have a property known as
                \textit{Iterables}. Similarly to a MapNode, a copy of
                the \textit{interface} will be made for each
                input. However, with \textit{iterables}, a copy of
                each dependent node will also be made.
		 
		\subsection{Limitations}
		Nipype, although meeting the various needs of
                researchers in Neuroinformatics \tristan{vague}, was
                not designed with the processing of Big Data in
                mind. Moreover, its performance relative to current
                Big Data frameworks remains unknown \tristan{You could
                  refer to your MS thesis here}.  \note{nipype
                  performs a lot of disk I/O. Recomputation of
                  workflows vs RDD caching}
		
    \section{PSOM} 
        \subsection{summary} 
        While Python is a popular programming language amongst
neuroinformaticians, it is not the only one that is commonly used. To address
the needs of the Matlab/Octave community, PSOM (Pipeline System for Octave and
Matlab) was developed. Similarly to Nipype, PSOM is easy-to-use, can run in
distributed environments and can interface with tools written in other
languages.

        \subsection{pipeline description}
        A pipeline in PSOM, is a series of connected jobs, which can be
represented as a (static?) directed acyclic graph. The pipeline itself is implemented 
using the Matlab/Octave data type. Each job within the pipeline
is assigned a unique name. There is only one mandatory field in the job
description (command). The remaining four optional fields include the list input
files (files\_in), the list output files (files\_out), the list of files to be
deleted after execution (files\_clean) and any Octave or Matlab datatype
containing the remaining parameters necessary for job execution (opt).
Jobs in PSOM run in a protected environment where they only have access to the
parameters that are passed. 

        In order to determine whether the opt field was correctly set, the
generic function \textit{psom\_struct\_defaults} may be applied. This function
will set the default values, ensure all mandatory fields were supplied and issue
warnings for all unidentified attributes.

        To define modules in PSOM, a special Octave/Matlab function type, known as "bricks"
is used. Bricks employ the same parameters as jobs but lack the file\_clean
parameter. All bricks contain an opt.flag\_test boolean flag, which, if set to
true, allows the user to perform a dry-run of the brick. A dry-run of a brick
consists of updating the default parameters and validating that the brick can
successfully execute with the parameters provided. Bricks also have an option to
specify the output folder (opt.output\_folder) for brick outputs to be stored.

        The pipeline generator is an easy way to create complete pipelines with
minimal information provided. A pipeline generator may accept at most the
file\_in and opt arguments. It then builds the pipeline based on this
information and the default parameters specified in each brick applied by the
pipeline generator.

        \subsection{pipeline execution}
        
        To execute a pipeline, the user must provide the pipeline structure and
the pipeline configuration options. The latter consists of the log
directory, which must be provided, and the pipepline execution modes (e.g., session, background, batch,
qsub, msub), in which the default mode is \textit{background}. Pipeline
execution is achieved through the execution of the \textit{psom\_run\_pipeline}
command. 

        Pipeline execution is broken down into three distinct modules: 1)
initialization stage, 2) pipeline manager and 3) job manager. The initialization
stage performs some checks to ensure that the pipeline can execute successfully
(e.g., verify that pipeline is a DAG, check that output files are generated only
once, check that all input files not generated by the pipeline exist on disk,
delete any already existing output files), as well as performance checks (e.g.
do not recompute pipeline steps that have already been executed, create
necessary output folders to avoid creating them for each individual job). The
initialization manager can determine which jobs need to be executed by comparing
the current pipeline jobs with the details of those previously executed stored
in the logs folder. The jobs that have been executed can have the following
states: 1) none, 2) finished and 3) failed. The initialization manager stage the jobs 
with a \textit{none} or \textit{failed} status for recomputation. For jobs whose
description has changed since the previous \textit{finished} execution, the
initialization will also stage it for recomputation. Should the user want to
reexecution a previously executed \textit{finished} job, the user may force
reexecution by setting \textit{opt\_pipe.restart} parameter to include all jobs
to be reexecuted.

        The pipeline manager is responsible for monitoring the state of the
pipeline and submitting jobs for execution. Its duties begin after the
initialization manager has completed execution. Jobs can be submitted by the
pipeline manager once all their dependencies have been satisfied. The
dependencies of a job are designated as met if there are none or when its parent jobs within the
dependency graph are tagged as \textit{finished}. In addition, a job is
considered to be successfully completely when all the dependencies of its
children jobs have been satisfied. Depending on the pipeline configuration,
there may be a limit to how many jobs can run in parallel. Once a job has
finished executing, an empty tag file is created with a filename containing the
final state in the logs folder. The pipeline manager continuously parses through
the logs folder in search of these state files to delete and update the history
file with the job state. Both the pipeline manager and the jobs can be executed
within the current session, locally in a independent session, or in an
independent session on a cluster

        
        Every time a job is submitted by the pipeline manager for execution, a
job manager is started. It is a Matlab function responsible for creating the job
profile and generating the job-specific logs and the job exit tags. When
the job terminates, the job manager will verify that all output has been
correctly generated and assigns a job tagged based on this verification that
will be reported back to the pipeline manager.

        \subsection{debugging and provenance}

        The information stored in the logs folder is complete with details
allowing for both debugging and provenance. The logs folder contains six files:
1) PIPE\_history.txt, 2) PIPE\_jobs.mat, 3) PIPE\_status.mat, 4) PIPE\_logs.mat,
5) PIPE\_profile.mat and 6) PIPE.mat. PIPE\_history.txt includes the execution
history of the pipeline manager. PIPE\_jobs.mat and PIPE\_profile.mat are both
Matlab/Octave files containing each executed job as a Matlab variable. They
differ in that PIPE\_jobs.mat contains the latest executed jobs and can be used
to reexecute the pipeline at any moment in time and PIPE\_profile.mat contains
the execution time of each job \note{i think both can be used to reexecute
pipeline from scratch. Also unclear whether PIPE\_profile.mat contains more than
just latest jobs}. It is also possible to access any of the previously 
used parameters through PIPE\_jobs.mat. PIPE\_status.mat stores the status of each 
job as a string variable, whereas PIPE\_log.mat stores each job's log as a string 
variable. Job logs, as generated by the log manager, consist of a text copy of the 
execution, in addition to the user name, date, time and operating system used to 
execute the job. Finally, all configuration details are stored in PIPE.mat. 
       
        \subsection{limitations of PSOM}
    \section{SPM and matlabbatch} 
        \note{This section may not make much sense. Have not really yet fully
understood matlabbatch}
        \subsection{summary} 
        SPM is a very popular neuroimaging toolkit, and as a result,
neuroimaging pipeline engines such as Nipype provide interfaces for it. SPM is
also equipped with its own neuroimaging pipeline engine known as
\textit{matlabbatch}. Similarly to PSOM and as its name suggest, it is written
in Matlab.

        \subsection{pipeline description}
        Pipelines in matlabbatch consist of a series of connected data
processing steps known as "modules" \note{DAG?? probably but not explicitly
listed anywhere}. All SPM modules, helper modules and their respective
dependencies can be defined using SPM's GUI \note{can probably also be done
through command-line}.

        For any matlabbatch job, it is possible to create a skeleton of it. A
skeleton is a Matlab script containing a commented list of necessary inputs at
the top and all code necessary to initialize and run the script. Before running,
the skeleton will need to be configured with the input names and other
necessary parameters.

        All configuration options can be set/changed by modifying the
\textit{cfg\_$\langle type \rangle$ } files. 

        Defaults can be set within the configuration files, through a default
file, or in a \textit{.def} field for leaf nodes \note{terminal jobs???}. The
latter updates at the start of the new job execution whereas the former are
configured at the beginning of the SPM/matlabbatch startup.


        Virtual outputs represent the dependencies between jobs \note{unsure}.
They contain three paramaters of which only one is optional: sname, src\_output,
tgt\_spec. sname is the display name that will be stored in the dependecies
list, whereas src\_output is a subscript reference used to refer to the output
that is produced at runtime. The optional parameter, tgt\_spec, is a description
of the kind of inputs the outputs produced can be a dependency for. The virtual
output callbacks are evaluated when cfg\_(ex)branch, cfg\_choice and cfg\_repeat 
have the correct number of child nodes within all their in-tree nodes. 

        \subsection{pipeline execution}
        Pipeline execution can be achieved either through the provided GUI or by
the command-line interface. The GUI, however, requires significant user
interaction, as the user must repeat the "Runs" command as many times as
necessary to reexecute the command on multiple data. The CLI, however, avoids
this with the help of its \textit{spm\_jobman} interface.

        The first step to running a batch pipeline is to perform the
initialization. This may include configuring the \textit{spm\_defaults.m} code
to run in command line and executing the \textit{spm(’defaults’, MODALITY)}
(where MODALITY represents the modality used in the pipeline) and
\textit{spm\_jobman(’initcfg’)}.

        After the intialization is complete, a job may be executed using the
\textit{spm\_jobman(’run’, job[, input1, input2 ...])} command. This command
requires the job names as well as its necessary inputs specified in the correct
order. The job can be specified as a variable, the name of a script that will
generate a job variable or a cell list containing a mixture of scripts and job
variables. 
         
        \subsection{debugging and provenance}
        All batch scripts can be saved as a Matlab scripts. These scripts not
only contain the code that was executed, but also all input parameters. Should
any script fail in the process of executing, an error will be printed to the
Matlab console. If the user is using the GUI, the error output may also be saved
to a Matlab file.
    
        \subsection{limitations of SPM}
    \section{Nextflow}
        \subsection{summary}
        Nextflow is a POSIX-compatible pipeline engine for scientific workflows.
It extends the Groovy programming language to provide an  intuitive domain-specific language (DSL) 
that enables users to easily describe their workflows. Unlike pipeline engines such as Nipype and
PSOM, Netflow does not employ DAG. It instead adheres to the
\textit{Dataflow} programming model which decides how data will flow through the pipeline at
runtime \note{fix this sentence}. Nextflow prides itself on allowing the
integration of containerized tools to address the issue of numerical instability
that may arise from different environment setups.
  
        \subsection{Pipeline description}
        A Nextflow pipeline is represented by the flow of data between processes. These
processes can be written in any Linux-compatible scripting language. As in Nipype, 
these processes are executed in an isolated environment. The only means
of communication between processes is through asynchronous FIFO queues known as
\textit{channels}. Each process can contain any number of input and output
channels. Through the use of channels, input and output can be stored not only
on disk but also in-memory \note{Nipype can also return outputs that are not on
disk, so this is nothing special}.  

        As with other pipeline engines (nipype, psom, etc.,), the order in which
the processes appear is irrelevant. Execution order is determined by the dependencies.        

        \subsection{Pipeline execution}
        The \textit{executor} is responsible for the execution of a Nextflow
pipeline. By default, it assumes that the pipeline is to be run locally. The
local executor will use multithreading in order to exploit available cores
within the local environment. To configure the executor to run in a remote distributed environment, it is
necessary to modify the configuration file to specify the environment. By
following this approach, Nextflow users can seamlessly run their code on various
processing platforms with a simple modification to a configuration file. In
addition to local, Nextflow currently has executors for SGE, LSF, SLURM,
PBS/Torque, NQSII, HTCondor, DRMAA, Ignite, Kubernetes and AWS Batch.

        \subsection{Debugging and Provenance}
        When executing a pipeline, it is always possible to specify the
generation of a execution report. The report is written in HTML and consists of three sections:
Summary, Resources and Tasks. Summary provides details on the execution command,
execution status, overall execution time, in addtion to other metadata. The
Resources tab, on the other hand, contains several HighChart plots representing
CPU, memory, time and disk read/write rate of each pipeline process. The final
section, Tasks, contains many metrics including the status and command script of
each task. 

    Other reports that can be generated at runtime include the Trace report and
the Timeline report. The Trace report include details such as CPU usage, start
time, submission time, etc, of each process that was executed, whereas, the
Timeline report generates an HTML timeline of all processes that have been
executed.        

    \note{doesn't seem to be anything on provenance...}

    \section{FastR}
        \subsection{summary}
        FastR is another Python-based neuroimaging pipeline framework that
provides added features such including access to different versions of the same
command-line tool \note{criticizes nipype for not having such a feature, but
nipype can access boutiques tools...so...} and compatibility with various
possible filesystems. In addition to its added features, FastR also guarantees
a simple-to-use framework for building pipelines written in any languages, has
built-in provenance support, provides the seamless ability to parallelize
pipelines with the use of plugins and avoids the numerical instability issue with
the simple integration of containerized pipelines. Similary to many other
pipeline framework, it also employs a DAG to describe data flow.
 
        \subsection{Pipeline description}

        A pipeline in FastR is a \textit{Network} of connected \textit{Nodes} in
which the directed edges represent the flow of data.  Each \textit{node}
contains a JSON or XML template known as a \textit{Tool}. 

\textit{Tools}
describe the details of the underlying application that is to be executed at the
node. Tools consist of three components: general metadata, target and interface.
The general metadata provides details on the Tools such as name, version,
authors, etc, whereas the target specifies details on the execution environment.
The interface, on the other hand, provides details on the inputs and outputs of
the tool. 

A JSON schema is required to validate the python object generated by the Tool. 
Fastr currently provides two schemas for object validation: the \textit{Tool} schema
and the \textit{FastrInterface} schema. It is, however, possible to add a custom
schema to FastR and define an \textit{Interface} for it. 

The \textit{Interface} class is responsible for executing a call to the command line tool.
It dictates the tags that must be defined within the interface tag of the
\textit{Tool}. For instance, the generic interface class requires details on the
input and output's id, cardinality (number of input values) and DataType (a
python object denoting the details of the Input or Output's data type).
DataTypes can also be grouped, allowing an inputs containing various files types
to be inputted to applications which can support the file types.
Verification is done by FastR to determine if the linked Input and Output data
types are compatible.

Input samples can consist of one or more values, in which the output is also a
sample consisting of the same number of values. During the instances in which it
is desirable to apply the same task to multiple inputs, a multidimensional array of samples, known as a collection, can be supplied to a
\textit{Node} for processing. If there are multiple input samples, they can be
passed to a node in pairwise behaviour (e.g. 1-1) or cross-product behaviour
(e.g. nxn) \textit{need to better explain this}.  

Output samples may need to be expanded (i.e. a single sample of values becomes separate
samples) or collapsed (i.e. multiple samples become a single sample of values).
FastR provides data link flow directives for such operations.

In FastR, there are four types of \textit{Nodes}: normal Nodes, Sink Nodes,
Source Nodes and Constant Nodes. Data enters the network through Source or
Constant Nodes and exit through Sink Nodes. Constant nodes differ from Source
Nodes in that their input data are defined by the network \note{Not sure if
saying this correctly}. Normal nodes are responsible for processing the
data. Nodes are connected to each other through links that highlight the
dependencies and form the \textit{Network}. Once the \textit{Network} is
defined, it can be executed.

The data type of the Source Node and Sink Node must be defined in the
\textit{Network} definition.
The data is define at runtime by evaluation of the uniform resource identifier
(URI). Multiple URIs can be defined through the use of wildcards and searches. Plugins are used to read and write data to
different filesystems specified in the URI scheme. Currently, FastR supports
virtual file system (VSF), csv files and XNAT. Plugins to support other
filesystems can easily be added to FastR. In addition, plugins that can only
read, write, or perform searches can also be created.   


        \subsection{Pipeline execution}
        To commence pipeline execution, the \textit{Network}'s execution
function is called on the Submission host. FastR then analyses the network and
its dependencies and breaks
down the Network into chunks that can be further processed. From the chunks, the
execution order of the \textit{Nodes} is determined. 

During \textit{Node} execution, a job is created for each input/output
combination. Jobs are then executed on the execution host (i.e. local, cluster,
cloud), as determined by the execution engine plugin. FastR currently only
offers support for local and cluster plugins, however can easily be extended to
support other plugins. 

While the job is executing, input URIs are translated to their actual path/values,
the execution environment is set up by the \textit{Tool} and the \textit{Tool}
command is executed by the \textit{Interface}. When the \textit{Interface}
returns the results, the
output data is validated and translated to URI format. 

After job completion, the results are returned to the Submission Host which
reads the output and updates the network accordingly. Should a chunk be
completed, the next chunk will be processed with the updated information, until
all chunks have been processed and network execution is complete.

        \subsection{Debugging and Provenance}
        In FastR, a provenance document is generated for each resulting file
generated in the Prov-N format. Data recorded in the provenance documents include \textit{Tool}
versions, input/output paths/values and their respective checksums, start and
end time of execution for each \textit{job}, stdout and stderr logs, the end
status and the complete environment description.

    Similarly to Nipype, the FastR network can be visualized using graphviz.

         
    \section{OpenMOLE}
        \subsection{summary}
        OpenMOLE (Open MOdeL Experiment) is a domain agnostic pipelining
framework that focuses on simplifying workflow creation through its Domain
Specific Language (DSL) built on top of Scala. Through the use of its DSL, much
of the backend is abstracted from users. It also offers a GUI for users to build
workflows. Similarly to other existing pipelining
frameworks, OpenMOLE enables parallelization through plugins, meaning that the
workflow need not be changed between different distribution environments.
Moreover, OpenMOLE supports the deployment of tasks or groups of tasks on different
computing environments.
Another key feature of OpenMOLE is that it attempts to address the
reproducibility issue through the integration of the CARE archiver. CARE allows
for the execution of command-line applications, written in any language, and
attempts to simulate the environment in which the application was designed on.
OpenMOLE also provides embedded methods for parameter exploration.

        \subsection{Pipeline description}
       An OpenMOLE workflow consists of tasks connected by transitions. Each
task contains details such as input, output and optional parameters, in addition
to embedding the application that will be applied to the data. Input values are
generally supplied by the dataflow through an entry point known as
\textit{Sources} and output values are returned to the
dataflow through \textit{Hooks}.   

      Variables in OpenMOLE can be native types (e.g. int, string), files,
directories, any Java or Scala types, and user-defined types (i.e. Java or Scala
classes). The output type of an input supplied to another task should be the
same as the input type, or be in a child class of the input type.

      There exist five types of transitions in OpenMOLE: flat, divergent,
convergent, explorative and aggregative. A flat transition is one in which the output of one
task is the input to a single other task. On the other hand, a divergent
transition is one in which the output of one task serves as the input to several
other tasks. The inverse of a divergent task is a convergent task, in which
several tasks supply input to a single task. Explorative transitions allow for
multiple excutions of the same task on the sampling outputs produced by an
exploration task. Conversly, an aggregative transition gathers all the streams
produced by an explorative transition. It is also possible to add a stopping
condition to terminate all streams once a condition is satisfied.

      Tasks are not directly connected to each other by transitions. They are
encapsulated within \textit{capsules} who are connected to each other by
transitions. This permits the same task to be used multiple times in the
workflow by encapsulating them in different capsules.

    There also exists the notion of \textit{Input Slots} in OpenMOLE, which enables
cycles in the workflow. A task may have several input slots and will execute
each time that all the transitions belonging to the same input slots has passed
through.
 
                

        \subsection{Pipeline execution}

      In order for an OpenMOLE workflow to be executed, there must exist a
single encapsulated task. A task will execute once all its dependencies have
been met. 

    During execution of a task, the CARETask will decompress the CARE archive
containing the application and its dependencies and the application is deployed
within the confines of the decompressed archive. 

OpenMOLE relies on the exit code returned by the application in order to
determine whether to reschedule the task. It is possible to prevent task
rescheduling by setting the boolean flag \textit{errorOnReturnValue} to false.
    
      OpenMOLE workflows can be executed on a variety of platforms such as
remote servers, clusters and compute grids. As these execution hosts may not have all
the required dependencies to execute a task, tasks are archived using CARE. This
ensures that all the required applications and their dependencies can be shipped
and executed at runtime on any Linux host.


        \subsection{Debugging and Provenance}
    A CARETasks standard output and standard error are normally printed out to
OpenMOLES's console. For distributed executions, this might not be a practical
solution, in which case such information may be passed through
the dataflow using OpenMOLE variables.

    \section{LONI}
        \subsection{Summary}
        The LONI pipeline processing environment facilitates the development,
sharing and execution of pipelines. Workflow Activities, known as \textit{Modules} in
LONI, are described using Extensible Markup Language (XML), which can easily
shared between user groups. LONI runs on the Java platform, allowing it to be
platform independent and has support DRMAA clusters and symmetric
multiprocessing (SMP) systems. In order to
capture provenance information, LONI is also equipped with a provenance manager.
  
        \subsection{Pipeline description}
        LONI uses the dataflow programming model to represent the pipeline. Data
flows between \textit{Modules} through \textit{data pipes}. Sub-pipelines are
known and \textit{Pipelets}. Modules are defined by the input arguments/streams
and output arguments/stream. Arguments in LONI may be specific
filetypes, flags or values listed in the order in which the application
requires. The provided arguments may be required or optional and can also be
duplicated, if necessary. File inputs in the form of URLs are accepted by LONI
and will be downloaded at runtime. List of inputs may also be provided to process
multiple inputs with copies of the same module in parallel (batch processing). Conversely, it is
also possible to group list of inputs so as to process them all by the single
module \note{Need to express this better}. Input/Output arguments may be added
to the table of global variables for easy access throughout the pipeline. It is
possible to apply variable modifiers to the variables stored within the table.

Intermediary outputs as well as the file output are treated as temporary by
LONI. They are scheduled for deletion before a new pipeline is executed. It is,
however, possible to preserve these files. 
  
        \subsection{Pipeline execution}
        As a
module completes it execution, its output gets transmitted through the dataflow
and the dependent modules are signaled that their input data has arrived.
Modules who have met all their dependency requirements are ready for execution.
Should a module have no dependecies, it can be executed immediately. Modules
ready to be executed are placed in a FIFO launch queue to avoid straining the
platform. 

       LONI offers a built in client/server strategy that uses Java's remote
method invocation (RMI) libraries to communicate between servers. This enables
modules, pipelets or pipelines to be executed on servers different from the
client. It also enables full pipelines to consist of connected modules and
pipelets located on different servers. It is favorable to have such a
configuration in LONI, as the tools required for processing may not be available
on client server, but located in another. The client is responsible for
dictating all file transfers and module launches that are to occur. The client
authenticates with the servers through a one-way hash. Only files produced by
the pipeline are accessible to the client and only temporary files created by
the server may be written to the server.

        \subsection{Debugging and provenance}
        For each dataset processed in a LONI pipeline, there exists a corresponding
provenance (.prov) XML file. The \textit{Provenance Editor}, a standalone
platform-independent application, reads the metadata from the image headers,
extracts relevant provenance information from them and creates the provenance files. 
This data can be modified by the user in the to correct inaccuracies or append
additional details before being saved. During pipeline execution, the provenance
file will be edited by the LONI pipeline to include details on each processing
step, the environment in which the data was processed and tool binaries used. 

    \section{Automatic Analysis}
        \subsection{Summary}
        Automatic analysis (aa) is another Matlab-based modular workflow engine,
having started of as a workflow engine for SPM. User may define their pipeline
through a Matlab user script and specify the tasks in addition to their input
and output types through an XML module interface. Modules used in pipelines must
be integrated into aa which can be easily accomplished through XML interfaces.
aa workflows can be executed locally, or in parallel through qsub or condor.

        \subsection{Pipeline description}
        aa pipelines have two main components: 1) the Matlab user script and 2)
the XML tasklist. The user script is a script that defines which data will be
processed, any necessary parameter, and how it will be processed. aa provides an example user script,
which can be modified to meet user's needs. The entire pipeline in aa is defined
in a single structure, known as aap, which makes the task of record-keeping
simple.

        The XML tasklist is an ordered list of all the modules that will be applied to
the data, that is read by the user script. This list does not specify any
dependencies as those dependendecies can be inferred by the pipeline through
analysis of the module interfaces. Modules can be separated into two
sections: initialization and main. Modules listed in the initialization section
are to be executed for every task. Conversely, those modules liste in the main
section are only executed once for every task, unless otherwise specified.


        In order to ensure that the correct output data is being operated on,
each task is provided with its own unique folder. The aa engine will transmit
the necessary input data to the folder before task execution and also
acquiring the outputs after task completion. Output folders are numbered such
that if a given module is invoked multiple times, each of those folders are
assigned an incremental ordering corresponding to the order in which they have been
executed \note{sentence might not make sense}. Should all data be required for
each task in the tasklist, all the data will be copied to each task folder.


    Similarly to a pipeline, each module is comprised of an XML interface and a
Matlab source file. A module's XML file consist of specifications on required input
and output modalities, as well as the desired domain. One of the most important
properties include the domain at which the module operates. While there exist mainy domains for
modules and new ones can easily be created and integrated by the user, the most
prominently used ones include: "study", "subject", "session". As their name suggests, a
module with the domain "study" will only be executed once for each time it
appears in the tasklist, whereas "subject" executes the module for each subject
within the task and "session" executes once for each session within a subject. 
 
    In addition to module domains, another important module property is the
input and output data stream types. Data stream types denote what input and
output data type each module produces. Commonly used data stream
types include "epi", "structural" and "dicom\_header".

    Optional fields specified in a module include qsub, for estimates of
requirements for parallel processing,
and \textit{performanceoutput}, for garbage collection.  

   Analysis parameters can easily be configured in aa. Any variable defined by
the \textit{aaprecipe} command may be modified. Parameters can also be defined
for an instance of a module within the XML tasklist. Alternatively, it is also
possible to create an XML file that inherits parameters from another XML file
and overrides some or all of the parameters.

   To set defaults and site-specific configurations, it is possible to either
create a local config file or by creating another parameter default file which
inherits the original one.   

    aa also supports the creation of branched tasklist. This allows a section of
the pipeline steps to undergo different tasks or the same tasks in a different
order in one pipeline rather than having to create separate pipelines.

        \subsection{Pipeline execution}
        Once the definition of the user-script, XML tasklist and modules is
complete, the pipeline is ready for execution. Dependencies in aa are
established based on input and output data stream types. The aa engine attempts
to match the last task executed with a corresponding data stream type to the
current type. It is, however, possible to specify which module provides the
input to the current module by using fully qualified stream references.

        During execution, a map of all the modules that need to be executed and
their respective dependencies is generated. Modules that have completed contain
a flag file indicating their completion within their instance's root directory. Such
modules will not be re-executed by the engine.
Additionally, if it is required that a previous module is re-executed, the done flag is
removed from all dependent modules.  

        Should part of the processing be performed on a different server it is
possible to connect the pipelines together through an analysis script. The aa engine will be reponsible for
matching the data streams between the servers in order to avoid the copying of
data and modules.

        \subsection{Debugging and provenance}
        In order to retrace the steps of previous executions, aa keeps a
detailed record of the events that occurred. These detailes are stored in a
Matlab object \note{similarly to PSOM}, which permits reexcution of a previous
run or extraction of specific parameter values.

        Additionally, to verify the correctness of the results produced, aa
implements tools for quality control and comes with a dedicated module for
low-level quality control.
        
    \section{Make}
        \subsection{Summary}
        As the creation of workflow activities requires a certain level of
programming expertise, Make was proposed as a simpler alternative. Make is a
tool used for the development of directed acyclic dependency graphs. It is
available in all Linux and UNIX distributions, which relieves the users from
worflow engine installation. Additionally, Make is capable of offering other
features necessary to workflow engines, such as, reproducibility,
parallelization, fault-tolerance and quality control. The motivation behind Make
is essentially that the tools available for simple and efficient pipeline
creation already exist and there is no need to 'reinvent the wheel'. In
addition, the use of Make is not limited to the neuroimaging community, and thus
support for it is widely available. Make has also been adapted for use in other
programming languages such as SnakeMake for python

 
        \subsection{Pipeline description}
        Make pipelines are described in makefiles, which consist of a set of
rules which describe the flow of data. Rules defined in make can be define by three
components: targets, dependecies and recipes. The target is the output
(typically file) produced by execution of the recipe, whereas the dependecy
would be the input file from which the target would be created from. The target
is only modified if its last modified date is early than that of a depencies's.
A recipe, in make, is the set of shell commands used to create the target from
its dependencies. 

In order connect the rules together, it is possible to define "phony" targets.
Phony targets do not correspond to any actual file and will be generated once
all of their dependencies (targets of other rules) have been
created. Chaining multiple rules together will automatically generate the directed acyclic
graph that is the pipeline. This is contrary to other pipelines such as Nipype
and LONI, in which the user must explicitly state all connections \note{as
stated in paper}. Through the use of rules, it is easily possible to reexecute
only the desired parts of the pipeline.

Cleanup after rule execution can be defined using the phony target
clean.

To create different targets for the same dependency files organized in different
ways (e.g. subject/timpepoint timepoint/session), it is possible to use symbolic
links for the dependencies, however this may complexify the code. Otherwise, it
may be simpler to copy subject and session makefiles.

        \subsection{Pipeline execution}

        Compared to the previous workflow engines, Make pipeline variables are
only evaluated at time of use (lazy evaluation). In addition, targets are not
executed in the order they are specified (similarly to other pipeline engines).

Make files are also well-equipped to handle workflows with conditionals. Should
a conditional exist in a workflow, it will be evaluated during the creation of
the DAG (i.e. not at runtime) and only the required components will be executed. 

        As parallelism can be inferred from the dependency graph, parallel execution of a make pipeline can occur simply through the
addition of the -j flag for multicore machines, or by using qmake command to run
the pipeline on a cluster.        

        By default, make only rebuilds targets whose last modified preceedes
that of its dependencies. As such, when a task fails and is re-executed, only the targets that need
to be rebuilt will be rebuilt. It is possible to specify reexcution in Make such
that the task will be reexecuted in case of failure.
 
        \subsection{Debugging and provenance}
        As users of a preexisting Make pipeline may not be aware of the proper
usage of each target, it is necessary to provide a `help` module that is
adaptable to targets being added removed or modified. It is possible to define
in Make a macro for such a purpose that will provide users with usage
information when requested (e.g. calling make help).

    Debugging in Make can be rather difficult, particularly when executing code
in parallel. However, pipelines in Make are broken down into many targets which
can each be tested individually. In addition, Make provides built-in flags to
aid in debugging. These flags include -n, for printing out the commands without
any execution and -p to print out all the rules. In the instance where pattern
substistution is used, -p will also include the results obtained from the
pattern substitution. More recent versions of GNU Make also include the --trace
and --output-sync flags which display why each recipe was executed and print the
parallelized output in sequential order, respectively.


    Neuroimaging tools are typically equipped with quality assurance modules. As
a result, it is possible to utilize the quality assurance modules from these
tools to provide a complete report for the entire pipeline. The proposed
solution suggests to write all the quality assurance outputs from each tool to a single HTML
report with R Markdown or a script. The R Markdown or script, with the metrics
and images are listed as dependencies to the QA report target in Make.


    Only a basic form of data provenance has been implemented by the authors.
The implemented data provenance consisted of a makefile that calls a bash to
extract acquisition parameters from the raw input images. The makefile also
obtains tool versions from the installation directories and pipeline options
from their respective makefiles. All of these details are stored in a CSV file.
The makefile will then call an R Markdown script which will then generate an
HTML provenance report from the corresponding CSV file.

    

          
    \section{Pydiper}
        \subsection{Summary}
        atoms -  wrappers for distinct operations
        modules - link atoms together
        applications - command line interface to the pipeline
        \subsection{Pipeline description}
        \subsection{Pipeline execution}
        \subsection{Debugging and provenance} 
    \section{Use of containers in Workflow engines} 
        \subsection{use of containers in nipype, psom, spm}
        
       \section{other domains}
       	\subsection{pegasus}
		
\chapter{Domain nonspecific BigData frameworks} 
    \section{mapreduce}
        Summary of ~\cite{mapred}

        Typically, simple computations need to be performed on 
        increasingly growing large datasets. Processing of such 
        large datasets require parallelization. As a result, 
        these computations are complicated by details of 
        parallelization, fault tolerance, data distribution and 
        load balancing. The MapReduce library addresses this issue 
        by allowing the expression of simple computations while 
        abstracting the other details. The map and reduce paradigm 
        was selected for this purpose as it was already commonly 
        used in functional programming languages, such as Lisp, and 
        it was observed that many computations could be expressed 
        as such.

        The MapReduce library operates by first dividing the data 
        into manageable chunks (16 to 64MB). It then creates 
        multiple copies of the program and distributes them across
        nodes. There exists a single master node; the remainder are
        worker nodes. The master node is responsible for delegating 
        map and reduce tasks to the idle workers. There are two data
        structures found in the master. The first stores the state 
        (\textit{idle}, \textit{in-progress} or \textit{completed}) 
        of each \textit{map} and \textit{reduce} task, and the second
        stores the identity of the non-idle worker nodes. In contrast,
        the worker nodes are responsible for the execution of 
        \textit{map} or \textit{reduce} tasks. A worker node assigned
        a \textit{map} task will parse out the key-value pairs from 
        the input file and pass it to the \textit{map} function. The 
        intermediate keys produced by the \textit{map} function are 
        buffered in memory and periodically written to disk.The 
        locations of these intermediate files are sent to the master 
        who is responsible for forwarding these locations to the 
        reducer. A worker node assigned a \textit{reduce} task uses 
        remote procedure calls to obtain the intermediate local files
        created by the map tasks and sorts the keys such that identical
        keys are grouped together. Each unique key and its list of 
        values are then submitted to the \textit{reduce} task and the 
        output of the \textit{reduce} task is stored in a final output 
        file. Once all \textit{reduce} tasks have completed, the master
        resumes the program and the output is returned to the user.

        In order to ensure fault tolerance, the master regularly pings 
        the worker nodes. Should a worker node not respond after a certain
        delay, the master labels the worker as failed. The \textit{map} 
        tasks that were completed by the worker are all returned to their
        original state (\textit{idle}) as \textit{map} task data is stored
        locally. Both the completed and in-progress worker's \textit{map}
        tasks are subsequently reassigned to other workers. Only the failed
        worker's \textit{in-progress} \textit{reduce} tasks need to be 
        reassigned.

        The master node undergoes periodic checkpoints, such that the 
        master node can be reinitialized from its last checkpoint in 
        case of failure. Master node failure is, however, unlikely.

        As network bandwidth is a scarce resource, the master attempts 
        to schedule \textit{map} tasks closest to where the data is 
        located. it will first attempt to schedule a map task on the host
        that contains the data; should that option not be available, the
        master will attempt to schedule the \textit{map} task on a host 
        nearest to the data (e.g. same network switch)

        Ideally, the number of \textit{map} and \textit{reduce} tasks 
        should be much larger than the number of available worker nodes. 
        Having it as such improves both dynamic load balancing and recovery 
        time. The number of \textit{map} tasks used in practice typically 
        corresponds to the size of the input chunks, whereas the number of 
        \textit{reduce} tasks are a small multiple of the number of worker 
        nodes used.

        When nearing completion of the MapReduce program, it is possible for
        ``stragglers" (\textit{in-progress} tasks running on nodes with below
        average performance) to delay completion. As a measure to counteract 
        this, the master node assigns the same task to idle workers. The task 
        is then marked as completed as soon as either the primary or the 
        backups complete the task. This strategy has been found to 
        significantly improve performance when used with very large datasets. 
        \note{44\% speedup with their sort program}.

        A few extensions have also been added to the library in order to 
        improve user experience. An important extension is the introduction 
        of a \textit{combiner}, which enables the commutative and associative 
        reduce function to be applied to the same keys on the hosts where the 
        \textit{map} task was executed. Like the \textit{map} task, the 
        \textit{combiner} produces intermediate output which is sent to the 
        \textit{reducer}. This alleviates the amount of key-value pairs that 
        must be transferred over the network from the map host to the reduce 
        host, and as a result, improves performance. Other extensions include 
        the ability to create user-defined partitioning functions, a guaranteed 
        increasing key sort order, the ability for the user to specify input 
        types, an option to skip records that continuously result in execution 
        failure, the ability to produce auxiliary output files in \textit{map} 
        or \textit{reduce} task, live status updates stream to HTTP server,
        the ability to run all code on a single local machine, and a 
        \textit{Counter} class.
    \section{spark} 
        \begin{itemize} 
            \item based on map-reduce 
            \item in-memory processing 
            \item lazy evaluation 
            \item general framework 
        \end{itemize} 

    \subsection{Scientific workflows in Spark}
        \subsubsection{Introduction}
        emphasis on data rather than on interface. transformation describe how
    interfaces interact with the data. 
        \subsubsection{ariel's paper and thunders astronomy - }
        \subsubsection{Spark on hpc's or } 
            \note{large dataset for valduriez} Prior to
    examining how Big Data frameworks could be improved on HPC clusters, it is
    important to have a baseline of how it performs natively on an HPC cluster.
    Particularly, how it behave when processing scientific workflows. In a recent
    study done by \cite{valduriez}, they examined the scalability of Spark on an HPC
    cluster using a black-box scientific workflow. Five tests were performed in
    their study: 1) Measuring execution time when task and resources available
    increase proportionally, 2) Measuring the execution time of very short and short
    tasks, 3) Measuring the execution time of short (5s) to long tasks (120s), 4)
    Varying the number of tasks with fixed task durations, 5) Mixed task durations
    and varied number of tasks. Their results show Spark behaves as expected when
    tasks and resources are increased exponentially, and that Spark scales better
    than expected with tasks of longer durations. This is due to the fact that the
    scheduler is overloaded in the case of many short tasks in addition to their
    being a significant amount of I/O occurring between each task. Although I/O is
    problematic in both data center and HPC infrastructures, HPC is optimized for
    data transfer over the network rather than disk I/O, and therefore, such
    frequent I/O negatively impacts the system. However, many scientific workflows
    are composed of longer tasks, and therefore, Spark scales very well with
    scientific workflows in an HPC environment.
     \section{Use of containers with Big Data frameworks}
     \section{Hadoop Distributed file system} 
        \note{maybe should go in between mapreduce and spark} 

        The Hadoop Distributed File System (HDFS)\cite{hadoop} 
        is the filesystem component of Hadoop. The metadata in 
        HDFS is stored in on a dedicated server known as the 
        NameNode, whereas application data is distributed across 
        many servers referred to as DataNode. To ensure fault 
        tolerance, HDFS replicates the data across multiple 
        DataNodes (default replication factor: 3). Not only does 
        this ensure that the data is not lost in the event of node
        failure, but it also increases data transfer bandwidth, as
        the data can be accessed from multiple nodes, and thus, 
        there are more opportunities for computations to be 
        performed nearest to where the data is located. 

        The NameNode contains the namespace tree, a hierarchy of 
        directories and their files, as well as the mapping of the 
        split file blocks to DataNodes. The entire namespace is 
        stored in memory. Information on the directories and files,
        such as modification and access times, namespace and disk 
        quotes, are stored within \textit{inodes} on the NameNode. 
        The name system's metadata, \textit{inode} and file block 
        mapping, is known as the \textit{image}. Persistent record 
        of the image that is stored on the local file system are 
        known as \textit{checkpoint}. Locations of block replicas 
        are not stored in the checkpoint as they may change over 
        time. The \textit{journal} is a log of the modifications 
        made to the image. Both the checkpoint and the journal may 
        be copied across servers for increased durability. The 
        journal is played back during the NameNode restart in order 
        to restore the cluster.

        When a client requests to read data, it must first contact 
        the NameNode. The NameNode provides it with the replica 
        locations, and the client reads from the DataNode located 
        closest to it. When the client requests to write a file, it
        contacts the NameNode which selects the DataNodes that will 
        host the replicas. The client subsequently writes the data 
        directly to the DataNodes in a pipeline fashion (The data 
        gets propagated through each DataNode by being transferred
        from the nearest DataNode to it).

        There is only one NameNode assigned to each cluster, however, 
        each cluster can have multiple clients and execute multiple 
        tasks concurrently.


        A block replica stored on a DataNode consists of two files 
        stored on the local file system: the metadata and the data. 

        The DataNode connects to the NameNode during startup and 
        performs handshake to for DataNode namespace ID and 
        software version verification. Should either namespace ID 
        or software version not match those of the NameNode, the 
        DataNode shuts down.

        To ensure integrity of the system, a \textit{namespace ID} 
        is assigned to all nodes during formatting of the namespace. 
        DataNodes with different namespace IDs to the NameNode will 
        not be permitted to join the cluster. Should the DataNode
        have not been assigned a namespace ID, it will be permitted 
        to join the cluster. The cluster's namespace ID will then be 
        assigned to that DataNode.


        Following the handshake, the DataNode register with the 
        NameNode using their storage ID. The storage ID is a unique 
        ID generated after initial registration to the NameNode and 
        that is persistently stored on the DataNode. It permits
        identification of the DataNode in the event of an IP address 
        or port change. 
        

\chapter{Optimizing Big Data frameworks for High-Performance Computing}
	
    \note{precise the systems that will be used. both have distributed memory.
data centre is hpc cluster with slower network, because may span multiple racks
and domains} \note{variety of jobs on hpc whereas dedicated map-reduce jobs in a
data center} \note{talk about spark on cloud and the challenges associated with}
\note{talk about why this is important} \note{talk } High-Performance Computing
(HPC) clusters are extensively used by the scientific community to process
complex, computation intensive problems. Due to their importance in the
advancement of scientific research, it is necessary that Big Data frameworks
used to process scientific data are compatible with them. 
	
    Big Data frameworks were optimized for a commodity cluster.  Such clusters
are made up of nodes with homogenous storage connected by lower-end network
cables (e.g. Ethernet). As a result, moving data from local storage to memory
was much less costly in these infrastructures than moving data across the
network. Thus, Big Data frameworks implemented data locality in an attempt to
reduce costly network (horizontal) transfers and favour more performant local
storage to memory (vertical) transfers. HPC clusters, however, are not
necessarily commodity clusters. Large HPC clusters are commonly made up of
supercomputing nodes connected by a high bandwidth, low-latency network (e.g.
InfiniBand). Nodes may contain their own local storage (e.g. HDD, SSD, tmpfs)
and are all connected to a distributed file system (e.g GPFS, Lustre).  Due to
their employment of a high bandwidth low latency network, network transfers on
such an HPC cluster may not be as costly as that of a typical data center, and
it may in fact be more costly to do vertical transfers. Furthermore, HPC systems
may enforce a batch submission system. This required users to specify number of
nodes, cores and total processing time; requirements which are unknown to
MapReduce users.
	
    This chapter will examine the various efforts by the community to improve
Big Data framework performance on HPC clusters.
	
    \section{File system improvements}
	

    In paper \note{Scaling Spark on HPC Systems} it was found that Spark on a
Lustre backend performed significantly (4x) slower than that of a workstation
using local fast SSDs for storage. This is due to the fact that file system
metadata latency (more specifically, file open) is the determinant factor for
performance on HPC, whereas network dominates performance in a typical data
centre configuration. It was found by the authors that using either a local
in-memory filesystem or a filesystem mounted to a single Lustre file improved
performance up to a point where it matched that of a workstation. In addition,
use of a cache was found to reduce file system metadata operations, and thus
improve overall performance.  The issues related to using an in-memory
filesystem is that it is limited by the amount of physical memory available, and
a job will fail if physical memory runs out. Moreover, there is not persistence
or resilience of data as it is not saved to non-volatile storage (disk). Spark
fails with medium to large applications due to "lax garbage collection in block
and shuffle managers". To resolve these issues, the authors attempted to make
all executors share a descriptor pool. This allowed files that were already
opened by other executors to be accessed by the executor through the descriptor
rather than having to reopen it again. However, the node OS image is subject to
number of Inode constraints as well Lustre limits on number of open files for a
given job. Due to Inode constraints, a livelock may occur when descriptor pool
is at capacity and an executor is attempting to open a file. The authors also
created mounted a local filesystem backed by a Lustre file (lustremount) to
address the issues related to an in-memory file system \note{requires admin
privilege}. It was found that the use of an in-memory file system improved
performance by 7.7x and the lustremount improved performance by 6.6x. It was
also found that implementation of the filepool improved performance.  Another
issue observed by the authors is that of poor block management as a result of
memory constraints resulting in a significant amount of vertical movement. When
available memory is exhausted, the least recently used block is evicted by the
block-manager. When a call is made to this removed block, it will need to be
recomputed, and in the process blocks required for recomputation may also be
evicted and necessitate recomputation of those blocks, thus leading to a chain
of eviction and recomputation. To resolve this, marking intermediate RDDs as
persistent, such that their results are saved to storage instead of requiring
recomputation, may be applied.  \note{they didn't look at improving initial
application I/O, though believe that it could be improved with lustremount and
filepool. } \note{for each partition, a shuffle file is created and written to
as many times as there are partitions. An index file is also created which
contains shuffle file locations and offsets. Metadata access is therefore
O(partitions2)}
	
    \subsection{tachyon paper} I/O is a known bottleneck in Spark that is
exacerbated in HPC systems in which data is stored in a separate cluster outside
compute nodes. One possible solution used to mitigate the effects of the I/O
bottleneck is to store data in an in-memory filesystem rather than on disk.
Alluxio and Triple-H are two known examples of such filesystems.
	
    Alluxio, formerly known as Tachyon, is an in-memory file system designed to
improve read and write performance without impacting fault tolerance of the
system. As write performance is impeded by the need to replicate data, Alluxio
leverages the concept of lineage to recompute the output if data is lost.
Tachyon exhibits 110x faster write throughput than in-memory HDFS and 4.4x
faster end-to-end latency. 

    Alluxio has is made up of two layers: the lineage layer and the persistence
layer. The lineage layer is the layer that performs recomputations to obtain the
desired output. The persistence layer contains all the code used to regenerate
the data, in addition to any data that cannot be recovered through lineage
as it may be too large to fit in memory or has no lineage information stored.
	
    \subsection{triple-H}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
    %%%In other words, such frameworks would try to limit data movement, as
    %network cables are expected to have low bandwidth and high latency. In
    %addition, each node in a commodity cluster is expected to have access to
    %its own local storage homogenous to that of the rest of the cluster, thus
    %replication is necessary to ensure data availability. This is, however, not
    %necessarily the case for computing grids. Computing grids may have access
    %to Infinibands, which provide high bandwidth and low latency. Moreover,
    %grids may use heterogenous storage in addition to a parallel file system
    %such as Lustre. As a result, it may, in some cases, be faster to transfer
    %data over the network than to access the nearest copy of the data. As well,
    %data replication may burden a system with centralized storage as multiple
    %copies of the data would be created, taking a significant amount of
    %space.%%%
	
	
	
    %%%Big Data frameworks were not designed with high-performance computing in
    %mind. Big Data filesystems such as were intended to run in data centers
    %with homogenous storage (ex. HDDs, SDDs. etc,.). In such infrastructures,
    %data movement is expensive. To counteract this, strategies were employed to
    %avoid moving data around (data locality). In contrast, HPC clusters use
    %heterogenous storage (ex. a combination of HDD, SSD, Ram Disk and Parallel
    %file systems) with compute nodes connected by high-performance
    %interconnects. HPC nodes may therefore not all be equipped with the same
    %type of storage devices, some of which may be faster than others. Unlike
    %data centers, HPC compute nodes do not have local storage. Moreover, the
    %high-performance interconnects ensure low latency and high throughput of
    %the system. In some instances, it may be even more performant to move data
    %around rather than ensuring data locality. When a single-node HPC cluster
    %using the Luster parallel file system was compared to a workstation with
    %local SSD, it was found that Spark performed about 4x slower on the HPC
    %cluster \cite{Chaimov:2016}.  As HDFS is not conscientious of the
    %underlying infrastructure and expects a data center-type infrastructure, it
    %cannot achieve its full performance potential. %%%
	
    %%%\section{Spark in the cloud}
	
    %%%Using Spark against a typical HPC parallel file system such a Luster has
    %been found to impede scalability. This was found to be due the file system
    %metadata latency, which limits Spark scalability to
    %\textit{O}(10\textsuperscript{2}) cores \cite{Chaimov:2016}.
	
	

    %%%Compute Grids, interconnected networks of computing resources, are
    %heavily used in scientific research for the processing of data. Due to
    %their use and importance to the scientific community, it is necessary that
    %big data frameworks used for processing neuroimaging data are compatible
    %with them. Compute grids typically utilize heterogenous storage, such as a
    %mixture of SSD, HDD, RAM Disk and parallel file systems (e.g. Luster).
    %However, big data frameworks, such as Spark are designed to work
    %efficiently with homogenous storage. Such differences result in reduced
    %performance and inefficient storage use when big data frameworks are used
    %on HPC clusters. This chapter will review the difference techniques applied
    %to big data frameworks as well as HPC and grids to improve collocation of
    %both services.%%%
	
    %%%\subsection{Anatomy of Grid storage} %%Typical grids consist of a cluster
    %of data nodes possibly connected to local storage (e.g. RAM Disk, HDD,
    %SDD), but is also connected through a network to another cluster
    %representing a parallel file system such as Lustre. RAM Disk is the fastest
    %storage as all data is located in memory, whereas SSD provides slightly
    %slower, persistent storage, but is a good alternative for big data. HDDs
    %are the least performant form of local storage.%%% %%Lustre is a stateful,
    %object-based parallel file system. It consists of three components: 1) the
    %meta data server (MDS), 2) the Object Storage Server (OSS) and 3) the
    %Luster Network (LNET), which enables clients to communicate with each
    %other.%%%

	
	
	
    In an attempt to improve performance of HDFS on HPC clusters, Triple-H was
created.  \section{Why is use of such systems important} \section{Scheduling}
\subsection{HPC schedulers: SLURM/QSUB} \subsection{Big Data schedulers:
YARN/MESOS} \subsection{Multi-level scheduling w/ BigData schedulers}
\chapter{Conclusion}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Body of Thesis goes here.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\addcontentsline{toc}{chapter}{Bibliography} \bibliography{bibliography}
\bibliographystyle{ieeetr}
%\bibliography{abbr,chalin,common,larch,tn}  %place your .bib files here
%\bibliographystyle{alpha}                   %the bibliography style to use

\end{document}
