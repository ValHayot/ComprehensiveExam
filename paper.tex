\documentclass{report}                                                           
        
\usepackage{titlesec}
\titleformat{\chapter}
  {\normalfont\LARGE\bfseries}{\thechapter}{1em}{}
\titlespacing*{\chapter}{0pt}{3.5ex plus 1ex minus .2ex}{2.3ex plus .2ex}
\usepackage[left=2cm, right=2cm, top=2cm, bottom=2cm]{geometry}
\usepackage{hyperref} 
\usepackage{xcolor} 
\usepackage{ulem}                      
                                                                                 
\newcommand{\note}[1]{\textcolor{blue}{\textit{note}: #1}}                       
\newcommand{\tristan}[1]{\textcolor{red}{TG: #1}}                                
\newcommand{\weird}[1]{\uwave{#1}}  


\begin{document} 
    \title{Pipeline systems and infrastructure for the efficient
            and open processing of Big neuroimaging Data} 
    \author{Valerie Hayot-Sasson}
    \maketitle 
    
    \begin{abstract} Text of abstract.  
    \end{abstract} 
    \tableofcontents
    \chapter{Introduction}
        In recent years, the volume of neuroimaging data acquired has exceeded
        both the storage and computation capacity of a standard research 
        lab workstation. With the advancement of data sharing technologies, this 
        data has been made widely available, with the only factor impeding
        research on this data being access to infrastructure and software that 
        enables efficient processing on such infrastructure. Many research labs 
        do have access to high performance computing (HPC) clusters, however, 
        without efficient software to process this data, processing can take an 
        excessive amount of time. Ensuring efficient processing of data is 
        complex and likely beyond the knowledge of an average neuroimaging 
        researcher. As a result, frameworks for the efficient processing of 
        neuroimaging data need to be developed.

        Many neuroimaging workflows currently exist. Examples include
        Nipype, Pipeline System for Octave and Matlab (PSOM), LONI, SPM, FastR,
        Automated Analysis (AA) and Pydpiper\note{add citations here}. These 
        workflow engines aim to satify four criteria: 1) straightforward 
        workflow composition, 2) performance, 3) portability of the workflows 
        and 4) reproducibility of the analysis. While these workflow engines
        do tackle performance, it is mainly limited to minimizing computation
        time and does not consider data transfer times -- which are typically
        costly in Big Data settings. Therefore, these engines need to be adapted
        for the processing of neuroimaging data.

        Big Data frameworks differ from those available in neuroimaging as their
        focus is mainly on workflow composition and performance. Big Data 
        frameworks such as Apache Spark and Dask both aim to be easy-to-use,
        applicable to a wide variety of analyses and improve performance in the 
        processing of Big Data. These types of systems were primarily designed 
        to be efficient on commodity infrastructure in which transferring data
        over a network would be extremely costly.

        Currently, there exists no Big Data framework which is adapted for the
        processing of neuroimaging data. However, tools such as the Thunder 
        Project \note{add citation} allow for the processing of time-series
        neuroimaging data using Spark as the backend framework. Tools such as 
        these provide built-in functions to process the data. For users to 
        extend functionality, it is necessary for them to understand the backend
        framework. Other scientific workflow engines, such as Toil and Pegasus
        \note{add citations}, have incorporated features that enable more 
        performant workflow executions for scientific Big Data. They also enable
        the integration of Big Data frameworks as subpipelines within their 
        workflows. Despite integration with Big Data frameworks, it is not
        possible to create Big Data workflows using their API exclusively.

        A big limitation to adapting Big Data frameworks for neuroimaging 
        workflows is that these frameworks are tightly-coupled with the 
        infrastructure they were designed for -- commodity clusters. While
        it is possible for some research labs to be using either a local cluster
        or the cloud for their processing, many may only have access to HPC
        clusters. As a result of this, most available scientific workflow 
        frameworks have been designed for processing on HPC clusters. The 
        set-up of an HPC cluster is quite different from that of a commodity 
        cluster, and therefore, in order to effectively use Big Data frameworks
        for scientific computing, it is necessary to adapt the infrastructure 
        for processing on HPC.

        In section \ref{datasets}, we will briefly discuss the two types of 
        datasets that are giving rise to the neuroimaging big data. Section 
        \ref{platforms} will provide a brief introduction to the data-sharing
        and processing platforms available to researchers. We will conclude our 
        introduction on the differences between commodity and HPC infrastructure
        in section \ref{infrastructure}.

        As we wish to evaluate and compare scientific workflow frameworks with
        Big Data frameworks for the processing of neuroimaging big data, we will
        break down our analyses by the four principal features necessary for
        effective neuroimaging workflow engines. The workflow engines discussed
        will include SPM, LONI, Nipype, PSOM, FastR, AA, Pydpiper 
        for neuroimaging; Taverna, Pegasus, Make, OpenMOLE, Nextflow, Galaxy, 
        Toil for other scientific workflows; and MapReduce, Spark and Dask for
        Big Data Frameworks. Chapter \ref{workcomp} will evaluate these 
        workflows on the basis of workflow composition. In other words, we will
        look at the programming languages necessary to construct these workflows
        , how easy is it to add new tools and will these workflows be understood
        by the community. In Chapter \ref{performance}, we will proceed to 
        investigate the strategies employed by these frameworks to improve the 
        performance of their workflows. Chapter \ref{portability} will discuss
        the portability of the workflows written using these frameworks. 
        Workflow reproducibility will be then discussed in Chapter 
        \ref{reproducibility}.
        

        \section{Big neuroimaging Datasets}\label{datasets}
            Neuroimaging big data comes in two formats: 1) large images and 2) 
            large datasets. Large images consist of a singular images ranging 
            from 100s of gigabytes to terabytes in size. A well-known example of 
            such an image is the BigBrain \note{cite here}, a histological image 
            of the brain of a healthy 69 year-old man. At its highest resolution
            , it is 1x1x20$\mu$m or 1TB in
            size. Other examples of large images can be found in electron 
            microscopy (EM), polarized light images (PLI) and micro coherence 
            tomography (microCT) \note{need to do more research here}. Images at 
            such high resolution are important to researchers as they provide 
            insights into aspects not otherwise detectible in images at lower 
            resolutions. However, due to their size and lack of resources 
            available to process such images, research on this data remains 
            limited.

            In contrast, large datasets are consists of many images, typically 
            arising from multiple different subjects, that are too big to fit in 
            storage. The images alone being small enough for an average 
            researcher to process. Example of these datasets include the 
            UK Biobank, HCP Project, ADNI dataset,OpenfMRI, amongst many others. 
            These datasets are very large. For instance, the UK Biobank is 
            expected to exceed \note{X} Petabytes in size. As subsets
            of these datasets are manageable for the average researcher to 
            process, much analysis has gone into them. Moreover, unlike for 
            large images, toolkits have been developed for researchers to 
            process this data. However,the toolkits do not enable researchers to 
            process these entire datasets as a whole. Therefore, it is crucial 
            to develop efficient frameworks for the processing of these datasets 
            in order for researchers to harness the data in its entirety.

        \section{Open-science Platforms}\label{platforms}
            Efficient processing of workflows are only but a small portion of 
            the Big Data issue in neuroscience. Sharing and storing of datasets
            and tools, secure access to and configuration of multiple computing 
            sites and provenance capture of all activities must be managed by 
            the researcher. This puts a heavy burden on researchers as they 
            require some knowledge of all these components in order to manage it
            correctly. These platforms also play a vital role in promoting 
            research through the sharing of knowledge. Users can choose to share
            their data and pipelines with others, reducing the need for others 
            to collect their own data or to develop their own workflows. It 
            also promotes robustness of results as the data and workflows will 
            be shared and tested by multiple users.

            Access to HPC resources may not always be straight-forward as 
            configuration must be adapted to the execution site. This can 
            range from scheduling policies, to available file systems, to 
            environment and available libraries. Platforms relieve the user
            from having to know site-specific details by providing seamless
            execution on any cluster the Platform has knowledge of.

            Examples of popular neuroinformatic open-science platforms include 
            CBRAIN\note{cite paper}, OpenNeuro \note{cite poster} and Neurodata
            \note{cite prepring}. OpenNeuro and NeuroData primarily focus on
            cloud deployement whereas CBRAIN is configured for the deployment
            of workflows on HPC resources. All three of these systems allow 
            users to configure, manage, share and execute their workflows with 
            the help of a web-based GUI. Integration with compute resources is
            entirely abstracted by the platforms. These platforms additionally
            provide tools to visualize their data.

            As neuroimaging workflow engines are typically best suited for 
            compute-intensive workflows, the workflows accessible in these 
            platforms are not expected to be as suitable to process the 
            volume of data made accessible by these platforms. Since these 
            platforms make various infrastructures available to researchers, 
            workflow engines must be adapted to efficiently execute on the 
            infrastructure made available.
            
        \section{Infrastructure}\label{infrastructure}


    \chapter{Workflow composition}\label{workcomp}
        \section{Programming language}
        \section{Modularity}
        \section{Workflow sharing}
    \chapter{Performance}\label{performance}
        \section{Filesystems}
        \section{Scheduling}
        \section{Performance enhancement strategies}
    \chapter{Portability}\label{portability}
        \section{Containers}
    \chapter{Reproducibility}\label{reproducibility}
        \section{Provenance Tracking}
        \section{containerization}
    \chapter{Discussion/Conclusion}


    \addcontentsline{toc}
        {chapter}{Bibliography} 
        \bibliography{bibliography}
        \bibliographystyle{ieeetr}
\end{document}
