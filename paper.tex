\documentclass{report}                                                           
        
\usepackage{titlesec}
\titleformat{\chapter}
  {\normalfont\LARGE\bfseries}{\thechapter}{1em}{}
\titlespacing*{\chapter}{0pt}{3.5ex plus 1ex minus .2ex}{2.3ex plus .2ex}
\usepackage[left=1in, right=1in, top=1in, bottom=1in]{geometry}
\usepackage{hyperref} 
\usepackage{xcolor} 
\usepackage{ulem}                      
                                                                                 
\newcommand{\note}[1]{\textcolor{blue}{\textit{note}: #1}}                       
\newcommand{\tristan}[1]{\textcolor{red}{TG: #1}}                                
\newcommand{\weird}[1]{\uwave{#1}}  


\begin{document} 
    \title{Pipeline systems and infrastructure for the efficient
            and open processing of Big neuroimaging Data} 
    \author{Valerie Hayot-Sasson}
    \maketitle 
    
    \begin{abstract} Text of abstract.  
    \end{abstract} 
    \tableofcontents
    \chapter{Introduction}
        In recent years, the volume of neuroimaging data acquired has exceeded
        both the storage and computation capacity of a standard research 
        lab workstation. With the advancement of data sharing technologies, this 
        data has been made widely available, with the only factor impeding
        research on this data being access to infrastructure and software that 
        enables efficient processing on such infrastructure. Many research labs 
        do have access to high performance computing (HPC) clusters, however, 
        without efficient software to process this data, processing can take an 
        excessive amount of time. Ensuring efficient processing of data is 
        complex and likely beyond the knowledge of an average neuroimaging 
        researcher. As a result, frameworks for the efficient processing of 
        neuroimaging data need to be developed.

        Many neuroimaging workflows currently exist. Examples include
        Nipype, Pipeline System for Octave and Matlab (PSOM), LONI, SPM, FastR,
        Automated Analysis (AA) and Pydpiper\note{add citations here}. These 
        workflow engines aim to satify four criteria: 1) straightforward 
        workflow composition, 2) performance, 3) portability of the workflows 
        and 4) reproducibility of the analysis. While these workflow engines
        do tackle performance, it is mainly limited to minimizing computation
        time and does not consider data transfer times -- which are typically
        costly in Big Data settings. Therefore, these engines need to be adapted
        for the processing of neuroimaging data.

        Big Data frameworks differ from those available in neuroimaging as their
        focus is mainly on workflow composition and performance. Big Data 
        frameworks such as Apache Spark and Dask both aim to be easy-to-use,
        applicable to a wide variety of analyses and improve performance in the 
        processing of Big Data. These types of systems were primarily designed 
        to be efficient on commodity infrastructure in which transferring data
        over a network would be extremely costly.

        Currently, there exists no Big Data framework which is adapted for the
        processing of neuroimaging data. However, tools such as the Thunder 
        Project \note{add citation} allow for the processing of time-series
        neuroimaging data using Spark as the backend framework. Tools such as 
        these provide built-in functions to process the data. For users to 
        extend functionality, it is necessary for them to understand the backend
        framework. Other scientific workflow engines, such as Toil and Pegasus
        \note{add citations}, have incorporated features that enable more 
        performant workflow executions for scientific Big Data. They also enable
        the integration of Big Data frameworks as subpipelines within their 
        workflows. Despite integration with Big Data frameworks, it is not
        possible to create Big Data workflows using their API exclusively.

        A big limitation to adapting Big Data frameworks for neuroimaging 
        workflows is that these frameworks are tightly-coupled with the 
        infrastructure they were designed for -- commodity clusters. While
        it is possible for some research labs to be using either a local cluster
        or the cloud for their processing, many may only have access to HPC
        clusters. As a result of this, most available scientific workflow 
        frameworks have been designed for processing on HPC clusters. The 
        set-up of an HPC cluster is quite different from that of a commodity 
        cluster, and therefore, in order to effectively use Big Data frameworks
        for scientific computing, it is necessary to adapt the infrastructure 
        for processing on HPC.

        In section \ref{datasets}, we will briefly discuss the two types of 
        datasets that are giving rise to the neuroimaging big data. Section 
        \ref{platforms} will provide a brief introduction to the data-sharing
        and processing platforms available to researchers. We will conclude our 
        introduction on the differences between commodity and HPC infrastructure
        in section \ref{infrastructure}.

        As we wish to evaluate and compare scientific workflow frameworks with
        Big Data frameworks for the processing of neuroimaging big data, we will
        break down our analyses by the four principal features necessary for
        effective neuroimaging workflow engines. The workflow engines discussed
        will include SPM, LONI, Nipype, PSOM, FastR, AA, Pydpiper 
        for neuroimaging; Taverna, Pegasus, Make, OpenMOLE, Nextflow, Galaxy, 
        Toil for other scientific workflows; and MapReduce, Spark and Dask for
        Big Data Frameworks. Chapter \ref{workcomp} will evaluate these 
        workflows on the basis of workflow composition. In other words, we will
        look at the programming languages necessary to construct these workflows
        , how easy is it to add new tools and will these workflows be understood
        by the community. In Chapter \ref{performance}, we will proceed to 
        investigate the strategies employed by these frameworks to improve the 
        performance of their workflows. Chapter \ref{portability} will discuss
        the portability of the workflows written using these frameworks. 
        Workflow reproducibility will be then discussed in Chapter 
        \ref{reproducibility}.
        

        \section{Big Neuroimaging Datasets}\label{datasets}

            As a result of the rise in datasharing in neuroscience, neuroimaging
            big data has come to fruition. The field of neuroscience has now
            reached a bottleneck where there exists sufficient data to make
            impactful studies, but the software to process these datasets have 
            not evolved with the size of the data.

            Neuroimaging big data comes in two formats: 1) large images and 2) 
            large datasets. Large images consist of a singular images ranging 
            from 100s of gigabytes to terabytes in size. A well-known example of 
            such an image is the BigBrain \note{cite here}, a histological image 
            of the brain of a healthy 69 year-old man. At its highest resolution
            , it is 1x1x20$\mu$m or 1TB in
            size. Other examples of large images can be found in electron 
            microscopy (EM), polarized light images (PLI) and micro coherence 
            tomography (microCT) \note{need to do more research here.cite}. 
            Images at 
            such high resolution are important to researchers as they provide 
            insights into aspects not otherwise detectible in images at lower 
            resolutions. However, due to their size and lack of resources 
            available to process such images, research on this data remains 
            limited.

            In contrast, large datasets are consist of many images acquired from
            multiple subjects, which are too large to be processed in their
            entirety. Notable examples of these datasets include the 
            Alzheimer's Disease Neuroimaging Initiative (ADNI), OpenfMRI, 
            the Autism Brain Imaging Data Exchange (ABIDE), the Consortium for
            Reliability and Reproducibility (CoRR), the Human Connectome Project
            (HCP) and the UK Biobank, with the later
            two being flagship examples of large datasets in neuroimaging.

            The ADNI intiative is a collection of images (structural and 
            functional MRI, PET), urine,
            serum, cerebrospinal fluid biomarkers, as assessments (clinical and
            neuropsychometric) of 800 elderly subjects, which of whom 200 where
            healthy, 400 exhibited mild cognitive impairment and the remaining
            200 showed signs of Alzheimers. Imaging data is available both 
            pre- and post-processed form.

            OpenfMRI is a public repository originally conceived for the sharing
            of task-based fMRI data. Over time, it has grown to include EEG, MEG
            , resting-state fMRI (R-fMRI) and diffusion fMRI. Data made 
            available through
            OpenfMRI are obtained from both healthy and clinical individuals.
            Raw data must be made available for all datasets published to 
            OpenfMRI, however, preprocessed data may also accompany the raw data
            . At present, OpenfMRI houses data of a total 3372 subjects
            originating from 95 studies \note{obtained from website}.

            To promote the advancement of knowledge of Autism Spectrum Disorders
            (ASD), ABIDE was conceived. ABIDE is an open repository of 
            preprocessed data obtained from 1112 R-fMRI datasets.
            In addition to R-fMRI, datasets also contain the corresponding 
            structural MRI and phenotypic data. As data made available in ABIDE
            is preprocessed, standardized pipelines were developed for the 
            preprocessing of the images. Popular neuroimaging processing tools
            such as CIVET, ANTs, and FreeSurfer \note{cite} were used for the 
            preprocessing of structural data, and several other pipelines were
            used for the processing of functional data.

            The CoRR was development as initiative to study and improve the 
            reliability and reproducibility of functional connectomics. The 
            data made available by the Consortium consists of test-retest data 
            obtained from 33 datasets. The repository contains data from 1629 
            subjects, in which anatomical, R-fMRI, dMRI, CBF and ASL scans are 
            available \note{obtained from website}.


            The HCP initiative aimed to characterize the human connectome of 
            1200 healthy adults, ranging from 22 to 35 years of age, by studying
            twins and their non-twin siblings. Data collected for this initiave
            includes structural MRI, R-fMRI, task fMRI and dMRI. MRI data was
            collected using a 3T scanner for all participants, with a 200 
            participants additionally undergoing scans in a higher-resolution 7T
            scanner. MEG and EEG data is available for a subset of the subjects. 
            In addition to imaging data, behavioural and genetic data were also 
            acquired. To faciliate analysis for research groups, HCP made 
            available the raw data, minimally pre-processed data and processed 
            data. Pipelines were developed by the HCP for pre- and post-
            processing. As in ABIDE, these pipelines reused many commonly-used
            neuroimaging tools such as FreeSurfer and FSL for structural data
            and FSL and other tools for functional data. The total amount of
            data made available by the HCP reaches up to nearly 1 Petabyte.


            The UK Biobank is an initiative aimed at the detection of pre-
            symptomatic markers of diseases occuring in middle-aged to elderly 
            populations. The study will gather data from questionnaires, 
            physical and cognative measures and biological samples of 500,000
            individuals. 100,000 of these individuals will additionally be 
            subjected to imaging studies. Imaging modalities used include
            structural MRI, fMRI and dMRI of the brain heart and body; low-dose
            X-ray bone and joint scans; and ultrasound of the carotid arteries.
            As with other initiatives, the UK Biobank makes available both 
            raw and processed data. This data is expected to exceed 0.2 
            Petabytes. Since processed data is provided, the UK Biobank makes 
            available it own pipelines based on tools in FSL, but also plans to
            adapt HCP pipelines from cortical surface extraction in their images
            .

            All these initiave stress the importance of data-sharing and
            large-scale studies for the advancement of science. Many researchers
            are limited to the processing of subsets of the data available as 
            they lack compute and storage resources to process the data in its
            entirety. It is also important to note that many of the pipelines 
            developed by these initiatives made use of popular neuroimaging 
            toolboxes, such as FSL, FreeSurfer, ANTS, CIVET, rather that
            creating their own. Therefore, there is a great need for bigdata
            frameworks that can leverage existing tools for the efficient 
            processing of large-scale datasets. With the availability of such 
            tools, many more research labs will be able to perform
            higher-impact studies \note{reword}.


        \section{Open-science Platforms}\label{platforms}
            Efficient processing of workflows are only but a small portion of 
            the Big Data issue in neuroscience. Sharing and storing of datasets
            and tools, secure access to and configuration of multiple computing 
            sites and provenance capture of all activities must be managed by 
            the researcher. This puts a heavy burden on researchers as they 
            require some knowledge of all these components in order to manage it
            correctly. These platforms also play a vital role in promoting 
            research through the sharing of knowledge. Users can choose to share
            their data and pipelines with others, reducing the need for others 
            to collect their own data or to develop their own workflows. It 
            also promotes robustness of results as the data and workflows will 
            be shared and tested by multiple users.

            Access to HPC resources may not always be straight-forward as 
            configuration must be adapted to the execution site. This can 
            range from scheduling policies, to available file systems, to 
            environment and available libraries. Platforms relieve the user
            from having to know site-specific details by providing seamless
            execution on any cluster the Platform has knowledge of.

            Examples of popular neuroinformatic open-science platforms include 
            CBRAIN\note{cite paper}, OpenNeuro \note{cite poster} and Neurodata
            \note{cite prepring}. OpenNeuro and NeuroData primarily focus on
            cloud deployement whereas CBRAIN is configured for the deployment
            of workflows on HPC resources. All three of these systems allow 
            users to configure, manage, share and execute their workflows with 
            the help of a web-based GUI. Integration with compute resources is
            entirely abstracted by the platforms. These platforms additionally
            provide tools to visualize their data.

            As neuroimaging workflow engines are typically best suited for 
            compute-intensive workflows, the workflows accessible in these 
            platforms are not expected to be as suitable to process the 
            volume of data made accessible by these platforms. Since these 
            platforms make various infrastructures available to researchers, 
            workflow engines must be adapted to efficiently execute on the 
            infrastructure made available.
            
        \section{Infrastructure}\label{infrastructure}
            HPC infrastructure was designed durign a time when compute resources
            were scarce. As a result, resource managers for these systems employ
            strategies to efficiently use the compute resources available for 
            processing. However, with the advent of big data, there has been a 
            shift from compute-intensive to data-intensive workloads. That is, 
            with the accumulation of large datasets, data transfers have begun 
            to monopolize processing time, and with it software has been 
            designed to minimize data transfers.

            We define HPC systems as those that employ shared parallel 
            filesystem such as Lustre. This shared filesystem is the slowest of
            all storage devices as it is shared amongst all users of the system.
            Compute nodes in an HPC system may or may not contain any local 
            storage and network interconnects between nodes are typically fast.
            Resources managers used in HPC infrastructures typically employ 
            batch scheduling and execute heterogenous workflows. 
            Examples of schedulers used in HPC clusters 
            include SLURM, PBS, SGE and HTCondor \note{cite here}.

            Commodity infrastructure differs from HPC infrastructures in that
            it uses less-costly hardware. It follows a shared-nothing paradigm
            where the storage is not shared between nodes. That is all compute
            nodes are responsible for storing the data as well. File systems
            used in these clusters are distributed filesystems, such as the
            Hadoop Distributed File System (HDFS) and Alluxio \note{cite}.
            Fault-tolerance of the data is achieved through replication across
            nodes. As transferring data across the network is typically costly 
            in such infrastructures, frameworks designed to run on commodity 
            infrastructure attempt to limit the transfer of data across nodes. 
            Resource managers created specifically for these systems
            typically expect dedicated infrastructure, as is the case with 
            Yet Another Resource Negotiator (YARN), but can also execute 
            heterogeneous workflows, as can be seen with Mesos \note{cite}.

            Commodity clusters may also come in the form of cloud infrastructure
            . While the hardware employed is typically the same as other
            commodity clusters, the software used varies. Cloud infrastructures
            have the added benefit of virtualization, allowing users to 
            configure the infrastructure and software requirements to meet their
            needs. Cloud infrastructure also enables researchers to request
            dedicated resources without having to maintain these resources. One
            of the biggest limiting factors of cloud resources is the cost 
            associated with usage.

            Researchers typically have access to HPC infrastructure, but may
            also have access to commodity infrastructure in the format of a 
            local cluster or cloud. Although both neuroimaging frameworks and
            big data frameworks can be made to execute in either environments,
            the performance improvements that they bring are very much 
            dependent on infrastructure that they are executed on. For instance,
            neuroimaging workflows typically read and write to a shared file
            system at each workflow step. This is less of an issue in HPC 
            infrastructure as network bandwidth is typically better than on 
            commodity infrastructure. In contrasts, Big Data framework typically
            rely on there being some form of local storage. Employing a Big Data
            framework on HPC infrastructure may in turn result in less to no 
            data locality -- a strategy employed by Big Data frameworks to 
            minimize data movement across the network. Due to the variability in
            infrastructure available to researchers, a workflow framework good
            for neuroscience must be well adapted to handle these differences.

    \chapter{Workflow composition}\label{workcomp}

        Workflows, also known as pipelines, describe the flow of data from 
        activity to another. They are typically represented by a 
        Directed Acyclic Graph (DAG). Workflow activites, also referred to as 
        workflows stages, encapsulate a tool. Tools may either take the form of 
        a command-line tool or a custom script. The engines determine which
        workflow activity can be staged by determining if the stage dependencies
        have been satisfied.

        In order for a workflow engine to be successful, it must be simple to 
        compose workflows using that framework. New researchers should be able 
        to acquire the skills to compose worfklows in that framework without
        investing a significant amount of time in understanding it. Reuse of
        existing tools is also common occurrence in neuroinformatics and 
        workflow engines need to accomodate this by undertaking a modular 
        approach and being easily extensible. With the sharing of data also 
        comes the sharing of workflows. For labs to share their workflows with
        the community at large, these workflows must be easily understood by the
        community.

        In this Chapter, we will compare and contrast the workflow composition 
        of seven neuroimaging frameworks (SPM, LONI, Nipype, PSOM, FastR, AA and
        Pydpiper), seven scientific workflow engines stemming from other fields
        (Taverna, Pegasus, Make, OpenMOLE, Nextflow, Galaxy, Toil), and three
        BigData frameworks (MapReduce, Spark, Dask). We will use three axes to
        evaluate workflow composition. Section~\ref{lang} will compare the 
        framework on the basis of programming language used, whereas 
        Section~\ref{mod} will look at how easy it is to integrate existing 
        tools into the workflows. Section~\ref{sharing} will evaluate the ease
        at which their corresponding workflows can be shared.

        \section{Workflow language}\label{lang}
            The choice of workflow language (e.g. GUI, scripting language,
            programming language) greatly
            affects how easily it will get picked up by the community. Complex
            languages may mean greater efficiency of the the engines
            , but also result in extensive time spent on learning and developing
            the pipelines. Moreover, some researchers may not be well-versed in
            programming and may require very easy-to-use engines.

            To address this issue, some workflow engines have taken the route
            of employing a graphical user interface (GUI). This makes workflow 
            composition accessible to all researchers, including those with
            little to no programming background. The caveat to employing a 
            workflow engine with a GUI interface for workflow composition is
            that users are greatly limited by the tasks they can perform. 
            Workflow engines that employ a worfklow composition GUI include
            SPM, Taverna, OpenMOLE, LONI and Galaxy. \note{double-check that 
            none have a programming API.Loni does..i think SPM does too}

            As more experienced programmers may feel limited by a GUI, workflow
            engines have decided to provide users with the ability to describe
            their workflows in more flexible formats. Even those with GUIs for 
            workflow composition, such as LONI and OpenMOLE, allow more 
            experienced users to script their workflow. Six different 
            programming
            languages are used for describing scientific workflows: 1) Python, 
            2) MATLAB and Octave, 3) Shell, 4) XML, 6) Common Workflow Language 
            (CWL) and 6) Domain Specific Language (DSL). Workflows engines that 
            employ usage of Python include: Nipype, FastR, Pydpiper, Pegasus
            and Toil.
            Python is an excellent choice as a programming language for 
            scientific workflows, particularly neuroinformatics, due to its 
            widespread use within the community. Other advantages of 
            programming in Python including a wide array of available libraries,
            including neuroimaging-specific ones such as Nilearn and Nibabel
            \note{cite}, and an lower learning curve compared to other 
            programming languages. Additionally, Python is platform independent
            meaning it can be installed on any system. In terms of BigData 
            frameworks that employ Python, Dask is exclusively written in Python
            , whereas Hadoop MapReduce and Spark both have interfaces to Python
            while being written in Java and Scala, respectively.

            Another widely-used high-level programming language in the 
            neuroinformatics community is MATLAB/Octave. Workflow engines which
            use this language include: PSOM, SPM, and Automatic analysis. A 
            downfall to imposing MATLAB as programming language is that the 
            it is proprietary, thus imposing a fee on 
            anyone wishes to program in that language. To offset this, PSOM 
            ensure compatibility with the GNU Octave programming language, thus 
            removing the requirement to purchase software for workflow 
            composition. Similarly to Python, MATLAB and Octave are 
            high-level platform-independent languages. MATLAB, in particular,
            also provides a wide array of libraries, however, the software must
            be purchased in order to access them.

            Scientific workflows are typically composed in UNIX-based 
            environments. These environments typically have Make installed by
            default eliminating the need for users to install it manually.
            Furthermore, all users who use Linux environments will typically 
            need to write some form of Shell script. This is the basis to use 
            workflow engines, such as Make, as scientific workflow engines. 
            As Make is exclusive to UNIX environments, it is not 
            platform-independent. However, Make has the flexibility of being
            easily extensible and can interface directly with any command-line
            tool. Shell scripting, nevertheless, is generally not as intuitive
            as programming languages like Python and MATLAB.

            Composing workflows in XML is another option provided by some 
            workflow engines such as Pegasus and LONI. However, the use of a 
            markup language removes some of the flexibility. Users wanting to 
            include 
            custom scripts as workflow activities must do so by creating a 
            command-line interface for their custom script. Pegasus, however,
            does provide many interfaces to describe the pipeline in including
            Java, Python and Perl.

            Another alternative is to compose workflows using Common Workflow
            Language (CWL). This can be seen in workflow engines like Toil. 
            CWL is not a programming language, but a specification for workflows
            . Files can either be written in JSON or YAML formats. CWL is 
            based on workflow engines like Make, but differs in that tasks
            are isolated and inputs and outputs must be explicit. Similarly to 
            make, CWL is only available on UNIX platforms. \note{cite cwl 
            website}

            Domain Specific Languages (DSL) are also used to compose workflows.
            DSLs are programming languages that have been adapted to specific
            domains. Workflow engine examples include OpenMOLE, which was 
            derived from Scala, and Netflow, which is derived from the Apache 
            Groovy programming language. While both are written on top of the 
            Java platform, Nextflow is only compatible with UNIX-based 
            environments. These programming languages are not otherwise commonly
            used in neuroimaging, meaning that users must learn specific 
            languages that would not otherwise be used in their field.

            With the exeption of LONI, neuroimaging workflows tend to stick to
            commonly used programming languages such as Python and MATLAB/Octave
            . This is likely due to the fact that the learning curve to adapt to
            these languages is relatively low and users can harness the 
            flexibility of these languages. 
            
            MapReduce, Spark and Dask all provide interfaces to Python, making 
            them good candidates for neuroimaging workflow engines. In addition
            Spark also interfaces with R, another programming language commonly
            used in neuroimaging.




        \section{Modularity}\label{mod}
        \section{Workflow sharing}\label{sharing}
    \chapter{Performance}\label{performance}
        \section{Filesystems}
        \section{Scheduling}
        \section{Performance enhancement strategies}
    \chapter{Portability}\label{portability}
        \section{Containers}
    \chapter{Reproducibility}\label{reproducibility}
        \section{Provenance Tracking}
        \section{containerization}
    \chapter{Discussion/Conclusion}


    \addcontentsline{toc}
        {chapter}{Bibliography} 
        \bibliography{bibliography}
        \bibliographystyle{ieeetr}
\end{document}
