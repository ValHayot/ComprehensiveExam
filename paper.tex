\documentclass{report}

\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{ulem}

\newcommand{\note}[1]{\textcolor{green}{\textit{note}: #1}}
\newcommand{\tristan}[1]{\textcolor{red}{TG: #1}}
\newcommand{\weird}[1]{\uwave{#1}}

\begin{document}
\title{Pipeline systems and infrastructure for the efficient and open processing of Big neuroimaging Data}
\author{Valerie Hayot-Sasson}
\maketitle
\begin{abstract}
  Text of abstract.  
\end{abstract}
\tableofcontents
\chapter{Introduction}
intro here
\chapter{BigData and Open science in neuroimaging}
    \note{Likely will not be split in two separate sections}
    \section{large images}
        \begin{itemize}  
            \item BigBrain
            \item Micro CT 
            \item EM 
        \end{itemize}
    \section{large datasets}
    	\note{off the top of my head, but need to double check}
        \begin{itemize}
            \item HCP
            \item ADNI
            \item OpenFMRI
            \item BIDS/BIDS apps
        \end{itemize}
    An effective engine to process such data requires the following characteristics
        \begin{itemize}
            \item in-memory computing
            \item data locality
            \item lazy evaluation
        \end{itemize}
        \section{open-science /data sharing platforms for neuroimaging}
        		is open science just data sharing, computing. 
        		\subsection{Introduction}
		\subsection{CBRAIN}
		\subsection{OpenNEURO}
		\subsection{NeuroData}
\chapter{Workflow engines for neuroimaging data}
	\section{Definition of workflows/pipelines}
	\section{Nipype}
		\subsection{summary}
		Nipype\cite{nipype} is a popular neuroimaging pipelinening framework written in Python.
		It provides users with uniform access to the rich ecosystem of neuroimaging software
		libraries (e.g. SPM, FSL, Freesurfer) through its \textit{Interfaces}. \weird{Should users want to
		use their own custom tool, it is also possible to create an interface for that}. To execute 
		Python code in Nipype, the \textit{Function} interface can be used \tristan{for arbitrary code?}. The framework is 
		also easy to use, and thus does not contribute to the steep learning curve new
		researchers must face.
		
		A pipeline in Nipype, consists of a data analysis
                \textit{Workflow} of connected
                \textit{interfaces}. The workflow is a directed
                acyclic graph that connects the outputs of one
                interface to be the inputs of another. A workflow may
                also consist of connected workflows.

                \tristan{The text below is about workflow execution
                  while the text above is about workflow description,
                  this should be a new paragraph.}  Workflows may be
                executed in parallel either locally or an a cluster
                through the use of a plugin in the Workflow's run
                function. For debugging, Nipype uses \textit{graphviz}
                to generate a static graph representing the Nodes and
                their relationships. \tristan{debugging might be a
                  third theme, in addition to description and
                  execution. Provenance might go in debugging.}
		
		 Each \textit{interface} within the workflow must be
                 contained within a \textit{Node} or a
                 \textit{MapNode} object, the MapNode class being a
                 subclass of the Node class. Wrapping an interface in
                 such objects ensures that interfaces are executed
                 within a uniquely named directories, which in turn
                 enables provenance tracking. Node objects also hash
                 inputs, provide the ability to iterate over inputs
                 and cache results. Hash inputs are useful in the case
                 of Workflow recomputation, where Nipype will only
                 recompute Nodes whose inputs have changed since the
                 previous run.
		
		Inspired by the MapReduce paradigm, the MapNode copies
                the interface to each input and executes it
                independently. \tristan{How are inputs defined/combined?} A reduce is subsequently performed by
                the MapNode to return the output in the form of a
                list. This differs from a Node object which can only
                execute the interface on a single input at a time.
		
		To enable execution of different parameters on the
                input, Nodes have a property known as
                \textit{Iterables}. Similarly to a MapNode, a copy of
                the \textit{interface} will be made for each
                input. However, with \textit{iterables}, a copy of
                each dependent node will also be made.
		 
		\subsection{Limitations}
		Nipype, although meeting the various needs of
                researchers in Neuroinformatics \tristan{vague}, was
                not designed with the processing of Big Data in
                mind. Moreover, its performance relative to current
                Big Data frameworks remains unknown \tristan{You could
                  refer to your MS thesis here}.  \note{nipype
                  performs a lot of disk I/O. Recomputation of
                  workflows vs RDD caching}
		
    \section{PSOM} 
        \subsection{summary} 
        While Python is a popular programming language amongst
neuroinformaticians, it is not the only one that is commonly used. To address
the needs of the Matlab/Octave community, PSOM (Pipeline System for Octave and
Matlab) was developed. Similarly to Nipype, PSOM is easy-to-use, can run in
distributed environments and can interface with tools written in other
languages.

        \note{pipeline description}
        A pipeline in PSOM, is a series of connected jobs, which can be
represented as a directed acyclic graph. The pipeline itself is implemented 
using the Matlab/Octave data type. Each job within the pipeline
is assigned a unique name. There is only one mandatory field in the job
description (command). The remaining four optional fields include the list input
files (files\_in), the list output files (files\_out), the list of files to be
deleted after execution (files\_clean) and any Octave or Matlab datatype
containing the remaining parameters necessary for job execution (opt).
Jobs in PSOM run in a protected environment where they only have access to the
parameters that are passed. 

        In order to determine whether the opt field was correctly set, the
generic function \textit{psom\_struct\_defaults} may be applied. This function
will set the default values, ensure all mandatory fields were supplied and issue
warnings for all unidentified attributes.

        To define modules in PSOM, a special Octave/Matlab function type, known as "bricks"
is used. Bricks employ the same parameters as jobs but lack the file\_clean
parameter. All bricks contain an opt.flag\_test boolean flag, which, if set to
true, allows the user to perform a dry-run of the brick. A dry-run of a brick
consists of updating the default parameters and validating that the brick can
successfully execute with the parameters provided. Bricks also have an option to
specify the output folder (opt.output\_folder) for brick outputs to be stored.

        The pipeline generator is an easy way to create complete pipelines with
minimal information provided. A pipeline generator may accept at most the
file\_in and opt arguments. It then builds the pipeline based on this
information and the default parameters specified in each brick applied by the
pipeline generator.

        \note{pipeline execution}
        
        To execute a pipeline, the user must provide the pipeline structure and
the pipeline configuration options. The latter consists of the log
directory and the pipepline execution modes (e.g., session, background, batch,
qsub, msub).

        \note{debugging and provenance}

       
        \subsection{limitations of PSOM}
    \section{SPM} 
        \subsection{summary} 
        \subsection{limitations of SPM} 
    \section{Use of containers in Workflow engines} 
        \subsection{use of containers in nipype, psom, spm}
        
       \section{other domains}
       	\subsection{pegasus}
		
\chapter{Domain nonspecific BigData frameworks} 
    \section{mapreduce}
        Summary of ~\cite{mapred}

        Typically, simple computations need to be performed on 
        increasingly growing large datasets. Processing of such 
        large datasets require parallelization. As a result, 
        these computations are complicated by details of 
        parallelization, fault tolerance, data distribution and 
        load balancing. The MapReduce library addresses this issue 
        by allowing the expression of simple computations while 
        abstracting the other details. The map and reduce paradigm 
        was selected for this purpose as it was already commonly 
        used in functional programming languages, such as Lisp, and 
        it was observed that many computations could be expressed 
        as such.

        The MapReduce library operates by first dividing the data 
        into manageable chunks (16 to 64MB). It then creates 
        multiple copies of the program and distributes them across
        nodes. There exists a single master node; the remainder are
        worker nodes. The master node is responsible for delegating 
        map and reduce tasks to the idle workers. There are two data
        structures found in the master. The first stores the state 
        (\textit{idle}, \textit{in-progress} or \textit{completed}) 
        of each \textit{map} and \textit{reduce} task, and the second
        stores the identity of the non-idle worker nodes. In contrast,
        the worker nodes are responsible for the execution of 
        \textit{map} or \textit{reduce} tasks. A worker node assigned
        a \textit{map} task will parse out the key-value pairs from 
        the input file and pass it to the \textit{map} function. The 
        intermediate keys produced by the \textit{map} function are 
        buffered in memory and periodically written to disk.The 
        locations of these intermediate files are sent to the master 
        who is responsible for forwarding these locations to the 
        reducer. A worker node assigned a \textit{reduce} task uses 
        remote procedure calls to obtain the intermediate local files
        created by the map tasks and sorts the keys such that identical
        keys are grouped together. Each unique key and its list of 
        values are then submitted to the \textit{reduce} task and the 
        output of the \textit{reduce} task is stored in a final output 
        file. Once all \textit{reduce} tasks have completed, the master
        resumes the program and the output is returned to the user.

        In order to ensure fault tolerance, the master regularly pings 
        the worker nodes. Should a worker node not respond after a certain
        delay, the master labels the worker as failed. The \textit{map} 
        tasks that were completed by the worker are all returned to their
        original state (\textit{idle}) as \textit{map} task data is stored
        locally. Both the completed and in-progress worker's \textit{map}
        tasks are subsequently reassigned to other workers. Only the failed
        worker's \textit{in-progress} \textit{reduce} tasks need to be 
        reassigned.

        The master node undergoes periodic checkpoints, such that the 
        master node can be reinitialized from its last checkpoint in 
        case of failure. Master node failure is, however, unlikely.

        As network bandwidth is a scarce resource, the master attempts 
        to schedule \textit{map} tasks closest to where the data is 
        located. it will first attempt to schedule a map task on the host
        that contains the data; should that option not be available, the
        master will attempt to schedule the \textit{map} task on a host 
        nearest to the data (e.g. same network switch)

        Ideally, the number of \textit{map} and \textit{reduce} tasks 
        should be much larger than the number of available worker nodes. 
        Having it as such improves both dynamic load balancing and recovery 
        time. The number of \textit{map} tasks used in practice typically 
        corresponds to the size of the input chunks, whereas the number of 
        \textit{reduce} tasks are a small multiple of the number of worker 
        nodes used.

        When nearing completion of the MapReduce program, it is possible for
        ``stragglers" (\textit{in-progress} tasks running on nodes with below
        average performance) to delay completion. As a measure to counteract 
        this, the master node assigns the same task to idle workers. The task 
        is then marked as completed as soon as either the primary or the 
        backups complete the task. This strategy has been found to 
        significantly improve performance when used with very large datasets. 
        \note{44\% speedup with their sort program}.

        A few extensions have also been added to the library in order to 
        improve user experience. An important extension is the introduction 
        of a \textit{combiner}, which enables the commutative and associative 
        reduce function to be applied to the same keys on the hosts where the 
        \textit{map} task was executed. Like the \textit{map} task, the 
        \textit{combiner} produces intermediate output which is sent to the 
        \textit{reducer}. This alleviates the amount of key-value pairs that 
        must be transferred over the network from the map host to the reduce 
        host, and as a result, improves performance. Other extensions include 
        the ability to create user-defined partitioning functions, a guaranteed 
        increasing key sort order, the ability for the user to specify input 
        types, an option to skip records that continuously result in execution 
        failure, the ability to produce auxiliary output files in \textit{map} 
        or \textit{reduce} task, live status updates stream to HTTP server,
        the ability to run all code on a single local machine, and a 
        \textit{Counter} class.
    \section{spark} 
        \begin{itemize} 
            \item based on map-reduce 
            \item in-memory processing 
            \item lazy evaluation 
            \item general framework 
        \end{itemize} 
     \section{Use of containers with Big Data frameworks}
     \section{Hadoop Distributed file system} 
        \note{maybe should go in between mapreduce and spark} 

        The Hadoop Distributed File System (HDFS)\cite{hadoop} 
        is the filesystem component of Hadoop. The metadata in 
        HDFS is stored in on a dedicated server known as the 
        NameNode, whereas application data is distributed across 
        many servers referred to as DataNode. To ensure fault 
        tolerance, HDFS replicates the data across multiple 
        DataNodes (default replication factor: 3). Not only does 
        this ensure that the data is not lost in the event of node
        failure, but it also increases data transfer bandwidth, as
        the data can be accessed from multiple nodes, and thus, 
        there are more opportunities for computations to be 
        performed nearest to where the data is located. 

        The NameNode contains the namespace tree, a hierarchy of 
        directories and their files, as well as the mapping of the 
        split file blocks to DataNodes. The entire namespace is 
        stored in memory. Information on the directories and files,
        such as modification and access times, namespace and disk 
        quotes, are stored within \textit{inodes} on the NameNode. 
        The name system's metadata, \textit{inode} and file block 
        mapping, is known as the \textit{image}. Persistent record 
        of the image that is stored on the local file system are 
        known as \textit{checkpoint}. Locations of block replicas 
        are not stored in the checkpoint as they may change over 
        time. The \textit{journal} is a log of the modifications 
        made to the image. Both the checkpoint and the journal may 
        be copied across servers for increased durability. The 
        journal is played back during the NameNode restart in order 
        to restore the cluster.

        When a client requests to read data, it must first contact 
        the NameNode. The NameNode provides it with the replica 
        locations, and the client reads from the DataNode located 
        closest to it. When the client requests to write a file, it
        contacts the NameNode which selects the DataNodes that will 
        host the replicas. The client subsequently writes the data 
        directly to the DataNodes in a pipeline fashion (The data 
        gets propagated through each DataNode by being transferred
        from the nearest DataNode to it).

        There is only one NameNode assigned to each cluster, however, 
        each cluster can have multiple clients and execute multiple 
        tasks concurrently.


        A block replica stored on a DataNode consists of two files 
        stored on the local file system: the metadata and the data. 

        The DataNode connects to the NameNode during startup and 
        performs handshake to for DataNode namespace ID and 
        software version verification. Should either namespace ID 
        or software version not match those of the NameNode, the 
        DataNode shuts down.

        To ensure integrity of the system, a \textit{namespace ID} 
        is assigned to all nodes during formatting of the namespace. 
        DataNodes with different namespace IDs to the NameNode will 
        not be permitted to join the cluster. Should the DataNode
        have not been assigned a namespace ID, it will be permitted 
        to join the cluster. The cluster's namespace ID will then be 
        assigned to that DataNode.


        Following the handshake, the DataNode register with the 
        NameNode using their storage ID. The storage ID is a unique 
        ID generated after initial registration to the NameNode and 
        that is persistently stored on the DataNode. It permits
        identification of the DataNode in the event of an IP address 
        or port change. 
        
\chapter{Scientific workflows in Spark}
    \section{Introduction} 
    \section{ariel's paper and thunders astronomy - }
    \section{Spark on hpc's or } 
        \note{large dataset for valduriez} Prior to
examining how Big Data frameworks could be improved on HPC clusters, it is
important to have a baseline of how it performs natively on an HPC cluster.
Particularly, how it behave when processing scientific workflows. In a recent
study done by \cite{valduriez}, they examined the scalability of Spark on an HPC
cluster using a black-box scientific workflow. Five tests were performed in
their study: 1) Measuring execution time when task and resources available
increase proportionally, 2) Measuring the execution time of very short and short
tasks, 3) Measuring the execution time of short (5s) to long tasks (120s), 4)
Varying the number of tasks with fixed task durations, 5) Mixed task durations
and varied number of tasks. Their results show Spark behaves as expected when
tasks and resources are increased exponentially, and that Spark scales better
than expected with tasks of longer durations. This is due to the fact that the
scheduler is overloaded in the case of many short tasks in addition to their
being a significant amount of I/O occurring between each task. Although I/O is
problematic in both data center and HPC infrastructures, HPC is optimized for
data transfer over the network rather than disk I/O, and therefore, such
frequent I/O negatively impacts the system. However, many scientific workflows
are composed of longer tasks, and therefore, Spark scales very well with
scientific workflows in an HPC environment.

\chapter{Optimizing Big Data frameworks for High-Performance Computing}
	
    \note{precise the systems that will be used. both have distributed memory.
data centre is hpc cluster with slower network, because may span multiple racks
and domains} \note{variety of jobs on hpc whereas dedicated map-reduce jobs in a
data center} \note{talk about spark on cloud and the challenges associated with}
\note{talk about why this is important} \note{talk } High-Performance Computing
(HPC) clusters are extensively used by the scientific community to process
complex, computation intensive problems. Due to their importance in the
advancement of scientific research, it is necessary that Big Data frameworks
used to process scientific data are compatible with them. 
	
    Big Data frameworks were optimized for a commodity cluster.  Such clusters
are made up of nodes with homogenous storage connected by lower-end network
cables (e.g. Ethernet). As a result, moving data from local storage to memory
was much less costly in these infrastructures than moving data across the
network. Thus, Big Data frameworks implemented data locality in an attempt to
reduce costly network (horizontal) transfers and favour more performant local
storage to memory (vertical) transfers. HPC clusters, however, are not
necessarily commodity clusters. Large HPC clusters are commonly made up of
supercomputing nodes connected by a high bandwidth, low-latency network (e.g.
InfiniBand). Nodes may contain their own local storage (e.g. HDD, SSD, tmpfs)
and are all connected to a distributed file system (e.g GPFS, Lustre).  Due to
their employment of a high bandwidth low latency network, network transfers on
such an HPC cluster may not be as costly as that of a typical data center, and
it may in fact be more costly to do vertical transfers. Furthermore, HPC systems
may enforce a batch submission system. This required users to specify number of
nodes, cores and total processing time; requirements which are unknown to
MapReduce users.
	
    This chapter will examine the various efforts by the community to improve
Big Data framework performance on HPC clusters.
	
    \section{File system improvements}
	

    In paper \note{Scaling Spark on HPC Systems} it was found that Spark on a
Lustre backend performed significantly (4x) slower than that of a workstation
using local fast SSDs for storage. This is due to the fact that file system
metadata latency (more specifically, file open) is the determinant factor for
performance on HPC, whereas network dominates performance in a typical data
centre configuration. It was found by the authors that using either a local
in-memory filesystem or a filesystem mounted to a single Lustre file improved
performance up to a point where it matched that of a workstation. In addition,
use of a cache was found to reduce file system metadata operations, and thus
improve overall performance.  The issues related to using an in-memory
filesystem is that it is limited by the amount of physical memory available, and
a job will fail if physical memory runs out. Moreover, there is not persistence
or resilience of data as it is not saved to non-volatile storage (disk). Spark
fails with medium to large applications due to "lax garbage collection in block
and shuffle managers". To resolve these issues, the authors attempted to make
all executors share a descriptor pool. This allowed files that were already
opened by other executors to be accessed by the executor through the descriptor
rather than having to reopen it again. However, the node OS image is subject to
number of Inode constraints as well Lustre limits on number of open files for a
given job. Due to Inode constraints, a livelock may occur when descriptor pool
is at capacity and an executor is attempting to open a file. The authors also
created mounted a local filesystem backed by a Lustre file (lustremount) to
address the issues related to an in-memory file system \note{requires admin
privilege}. It was found that the use of an in-memory file system improved
performance by 7.7x and the lustremount improved performance by 6.6x. It was
also found that implementation of the filepool improved performance.  Another
issue observed by the authors is that of poor block management as a result of
memory constraints resulting in a significant amount of vertical movement. When
available memory is exhausted, the least recently used block is evicted by the
block-manager. When a call is made to this removed block, it will need to be
recomputed, and in the process blocks required for recomputation may also be
evicted and necessitate recomputation of those blocks, thus leading to a chain
of eviction and recomputation. To resolve this, marking intermediate RDDs as
persistent, such that their results are saved to storage instead of requiring
recomputation, may be applied.  \note{they didn't look at improving initial
application I/O, though believe that it could be improved with lustremount and
filepool. } \note{for each partition, a shuffle file is created and written to
as many times as there are partitions. An index file is also created which
contains shuffle file locations and offsets. Metadata access is therefore
O(partitions2)}
	
    \subsection{tachyon paper} I/O is a known bottleneck in Spark that is
exacerbated in HPC systems in which data is stored in a separate cluster outside
compute nodes. One possible solution used to mitigate the effects of the I/O
bottleneck is to store data in an in-memory filesystem rather than on disk.
Alluxio and Triple-H are two known examples of such filesystems.
	
    Alluxio, formerly known as Tachyon, is an in-memory file system designed to
improve read and write performance without impacting fault tolerance of the
system. As write performance is impeded by the need to replicate data, Alluxio
leverages the concept of lineage to recompute the output if data is lost.
Tachyon exhibits 110x faster write throughput than in-memory HDFS and 4.4x
faster end-to-end latency. 

    Alluxio has is made up of two layers: the lineage layer and the persistence
layer. The lineage layer is the layer that performs recomputations to obtain the
desired output. The persistence layer contains all the code used to regenerate
the data, in addition to any data that cannot be recovered through lineage
as it may be too large to fit in memory or has no lineage information stored.
	
    \subsection{triple-H}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
    %%%In other words, such frameworks would try to limit data movement, as
    %network cables are expected to have low bandwidth and high latency. In
    %addition, each node in a commodity cluster is expected to have access to
    %its own local storage homogenous to that of the rest of the cluster, thus
    %replication is necessary to ensure data availability. This is, however, not
    %necessarily the case for computing grids. Computing grids may have access
    %to Infinibands, which provide high bandwidth and low latency. Moreover,
    %grids may use heterogenous storage in addition to a parallel file system
    %such as Lustre. As a result, it may, in some cases, be faster to transfer
    %data over the network than to access the nearest copy of the data. As well,
    %data replication may burden a system with centralized storage as multiple
    %copies of the data would be created, taking a significant amount of
    %space.%%%
	
	
	
    %%%Big Data frameworks were not designed with high-performance computing in
    %mind. Big Data filesystems such as were intended to run in data centers
    %with homogenous storage (ex. HDDs, SDDs. etc,.). In such infrastructures,
    %data movement is expensive. To counteract this, strategies were employed to
    %avoid moving data around (data locality). In contrast, HPC clusters use
    %heterogenous storage (ex. a combination of HDD, SSD, Ram Disk and Parallel
    %file systems) with compute nodes connected by high-performance
    %interconnects. HPC nodes may therefore not all be equipped with the same
    %type of storage devices, some of which may be faster than others. Unlike
    %data centers, HPC compute nodes do not have local storage. Moreover, the
    %high-performance interconnects ensure low latency and high throughput of
    %the system. In some instances, it may be even more performant to move data
    %around rather than ensuring data locality. When a single-node HPC cluster
    %using the Luster parallel file system was compared to a workstation with
    %local SSD, it was found that Spark performed about 4x slower on the HPC
    %cluster \cite{Chaimov:2016}.  As HDFS is not conscientious of the
    %underlying infrastructure and expects a data center-type infrastructure, it
    %cannot achieve its full performance potential. %%%
	
    %%%\section{Spark in the cloud}
	
    %%%Using Spark against a typical HPC parallel file system such a Luster has
    %been found to impede scalability. This was found to be due the file system
    %metadata latency, which limits Spark scalability to
    %\textit{O}(10\textsuperscript{2}) cores \cite{Chaimov:2016}.
	
	

    %%%Compute Grids, interconnected networks of computing resources, are
    %heavily used in scientific research for the processing of data. Due to
    %their use and importance to the scientific community, it is necessary that
    %big data frameworks used for processing neuroimaging data are compatible
    %with them. Compute grids typically utilize heterogenous storage, such as a
    %mixture of SSD, HDD, RAM Disk and parallel file systems (e.g. Luster).
    %However, big data frameworks, such as Spark are designed to work
    %efficiently with homogenous storage. Such differences result in reduced
    %performance and inefficient storage use when big data frameworks are used
    %on HPC clusters. This chapter will review the difference techniques applied
    %to big data frameworks as well as HPC and grids to improve collocation of
    %both services.%%%
	
    %%%\subsection{Anatomy of Grid storage} %%Typical grids consist of a cluster
    %of data nodes possibly connected to local storage (e.g. RAM Disk, HDD,
    %SDD), but is also connected through a network to another cluster
    %representing a parallel file system such as Lustre. RAM Disk is the fastest
    %storage as all data is located in memory, whereas SSD provides slightly
    %slower, persistent storage, but is a good alternative for big data. HDDs
    %are the least performant form of local storage.%%% %%Lustre is a stateful,
    %object-based parallel file system. It consists of three components: 1) the
    %meta data server (MDS), 2) the Object Storage Server (OSS) and 3) the
    %Luster Network (LNET), which enables clients to communicate with each
    %other.%%%

	
	
	
    In an attempt to improve performance of HDFS on HPC clusters, Triple-H was
created.  \section{Why is use of such systems important} \section{Scheduling}
\subsection{HPC schedulers: SLURM/QSUB} \subsection{Big Data schedulers:
YARN/MESOS} \subsection{Multi-level scheduling w/ BigData schedulers}
\chapter{Current status of Big Data frameworks in neuroscience} \section{Ariel's
paper} \section{Thunder} \chapter{Conclusion}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Body of Thesis goes here.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\addcontentsline{toc}{chapter}{Bibliography} \bibliography{bibliography}
\bibliographystyle{ieeetr}
%\bibliography{abbr,chalin,common,larch,tn}  %place your .bib files here
%\bibliographystyle{alpha}                   %the bibliography style to use

\end{document}
