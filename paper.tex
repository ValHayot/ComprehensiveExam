\documentclass{report}

\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{ulem}


\newcommand{\note}[1]{\textcolor{green}{\textit{note}: #1}}
\newcommand{\tristan}[1]{\textcolor{red}{TG: #1}}
\newcommand{\weird}[1]{\uwave{#1}}

\begin{document}
\title{Pipeline systems and infrastructure for the efficient and open processing of Big neuroimaging Data}
\author{Valerie Hayot-Sasson}
\maketitle
\begin{abstract}
  Text of abstract.  
\end{abstract}
\tableofcontents
\chapter{Introduction}
intro here
\chapter{BigData and Open science in neuroimaging}
    \note{Likely will not be split in two separate sections}
    \section{large images}
        \begin{itemize}  
            \item BigBrain
            \item Micro CT 
            \item EM 
        \end{itemize}
    \section{large datasets}
    	\note{off the top of my head, but need to double check}
        \begin{itemize}
            \item HCP
            \item ADNI
            \item OpenFMRI
            \item BIDS/BIDS apps
        \end{itemize}
    An effective engine to process such data requires the following characteristics
        \begin{itemize}
            \item in-memory computing
            \item data locality
            \item lazy evaluation
        \end{itemize}
        \section{open-science /data sharing platforms for neuroimaging}
        		is open science just data sharing, computing. 
        		\subsection{Introduction}
		\subsection{CBRAIN}
		\subsection{OpenNEURO}
		\subsection{NeuroData}
\chapter{Workflow engines for neuroimaging data}
	\section{Definition of workflows/pipelines}
	\section{Nipype}
		\subsection{summary}
		Nipype\cite{nipype} is a popular neuroimaging pipelinening framework written in Python.
		It provides users with uniform access to the rich ecosystem of neuroimaging software
		libraries (e.g. SPM, FSL, Freesurfer) through its \textit{Interfaces}. \weird{Should users want to
		use their own custom tool, it is also possible to create an interface for that}. To execute 
		Python code in Nipype, the \textit{Function} interface can be used \tristan{for arbitrary code?}. The framework is 
		also easy to use, and thus does not contribute to the steep learning curve new
		researchers must face.
		
		A pipeline in Nipype, consists of a data analysis
                \textit{Workflow} of connected
                \textit{interfaces}. The workflow is a directed
                acyclic graph that connects the outputs of one
                interface to be the inputs of another. A workflow may
                also consist of connected workflows.

                \tristan{The text below is about workflow execution
                  while the text above is about workflow description,
                  this should be a new paragraph.}  Workflows may be
                executed in parallel either locally or an a cluster
                through the use of a plugin in the Workflow's run
                function. For debugging, Nipype uses \textit{graphviz}
                to generate a static graph representing the Nodes and
                their relationships. \tristan{debugging might be a
                  third theme, in addition to description and
                  execution. Provenance might go in debugging.}
		
		 Each \textit{interface} within the workflow must be
                 contained within a \textit{Node} or a
                 \textit{MapNode} object, the MapNode class being a
                 subclass of the Node class. Wrapping an interface in
                 such objects ensures that interfaces are executed
                 within a uniquely named directories, which in turn
                 enables provenance tracking. Node objects also hash
                 inputs, provide the ability to iterate over inputs
                 and cache results. Hash inputs are useful in the case
                 of Workflow recomputation, where Nipype will only
                 recompute Nodes whose inputs have changed since the
                 previous run.
		
		Inspired by the MapReduce paradigm, the MapNode copies
                the interface to each input and executes it
                independently. \tristan{How are inputs defined/combined?} A reduce is subsequently performed by
                the MapNode to return the output in the form of a
                list. This differs from a Node object which can only
                execute the interface on a single input at a time.
		
		To enable execution of different parameters on the
                input, Nodes have a property known as
                \textit{Iterables}. Similarly to a MapNode, a copy of
                the \textit{interface} will be made for each
                input. However, with \textit{iterables}, a copy of
                each dependent node will also be made.
		 
		\subsection{Limitations}
		Nipype, although meeting the various needs of
                researchers in Neuroinformatics \tristan{vague}, was
                not designed with the processing of Big Data in
                mind. Moreover, its performance relative to current
                Big Data frameworks remains unknown \tristan{You could
                  refer to your MS thesis here}.  \note{nipype
                  performs a lot of disk I/O. Recomputation of
                  workflows vs RDD caching}
		
	\section{PSOM}
		\subsection{summary}
		\subsection{limitations of PSOM}
	\section{SPM}
		\subsection{summary}
		\subsection{limitations of SPM}
	\section{Use of containers in Workflow engines}
		\subsection{use of containers in nipype, psom, spm}
		
\chapter{Domain nonspecific BigData frameworks}
	\section{mapreduce}
		\begin{itemize}
			\item many computations can be described through map and reduce
			\item fault tolerant
		\end{itemize}
	\section{spark}
		\begin{itemize}
			\item based on map-reduce
			\item in-memory processing
			\item lazy evaluation
			\item general framework
		\end{itemize}
	\section{Use of containers with Big Data frameworks}
	\section{Hadoop Distributed file system}
		\note{maybe should go in between mapreduce and spark}
		\begin{itemize}
			\item enables data-locality in Spark/ decouples data locality logic from mapreduce\
			\item data replication
			\item fault-tolerance
		\end{itemize}
\chapter{Scientific workflows in Spark}
	\section{Introduction}
	\section{ariel's paper and thunders astronomy - }
	\section{Spark on hpc's or }
	\note{large dataset for valduriez}
	Prior to examining how Big Data frameworks could be improved on HPC clusters, it is important to have a baseline of how it performs natively on an HPC cluster. Particularly, how it behave when processing scientific workflows. In a recent study done by \cite{valduriez}, they examined the scalability of Spark on an HPC cluster using a black-box scientific workflow. Five tests were performed in their study: 1) Measuring execution time when task and resources available increase proportionally, 2) Measuring the execution time of very short and short tasks, 3) Measuring the execution time of short (5s) to long tasks (120s), 4) Varying the number of tasks with fixed task durations, 5) Mixed task durations and varied number of tasks. Their results show Spark behaves as expected when tasks and resources are increased exponentially, and that Spark scales better than expected with tasks of longer durations. This is due to the fact that the scheduler is overloaded in the case of many short tasks in addition to their being a significant amount of I/O occurring between each task. Although I/O is problematic in both data center and HPC infrastructures, HPC is optimized for data transfer over the network rather than disk I/O, and therefore, such frequent I/O negatively impacts the system. However, many scientific workflows are composed of longer tasks, and therefore, Spark scales very well with scientific workflows in an HPC environment.

\chapter{Optimizing Big Data frameworks for High-Performance Computing}
	
	\note{precise the systems that will be used. both have distributed memory. data centre is hpc cluster with slower network, because may span multiple racks and domains}
	\note{variety of jobs on hpc whereas dedicated map-reduce jobs in a data center}
	\note{talk about spark on cloud and the challenges associated with}
	\note{talk about why this is important}
	\note{talk }
	High-Performance Computing (HPC) clusters are extensively used by the scientific community to process complex, computation intensive problems. Due to their importance in the advancement of scientific research, it is necessary that Big Data frameworks used to process scientific data are compatible with them. 
	
	Big Data frameworks were optimized for a commodity cluster.  Such clusters are made up of nodes with homogenous storage connected by lower-end network cables (e.g. Ethernet). As a result, moving data from local storage to memory was much less costly in these infrastructures than moving data across the network. Thus, Big Data frameworks implemented data locality in an attempt to reduce costly network (horizontal) transfers and favour more performant local storage to memory (vertical) transfers. HPC clusters, however, are not necessarily commodity clusters. Large HPC clusters are commonly made up of supercomputing nodes connected by a high bandwidth, low-latency network (e.g. InfiniBand). Nodes may contain their own local storage (e.g. HDD, SSD, tmpfs) and are all connected to a distributed file system (e.g GPFS, Lustre).  Due to their employment of a high bandwidth low latency network, network transfers on such an HPC cluster may not be as costly as that of a typical data center, and it may in fact be more costly to do vertical transfers. Furthermore, HPC systems may enforce a batch submission system. This required users to specify number of nodes, cores and total processing time; requirements which are unknown to MapReduce users.
	
	This chapter will examine the various efforts by the community to improve Big Data framework performance on HPC clusters.
	
	\section{File system improvements}
	

	In paper \note{Scaling Spark on HPC Systems} it was found that Spark on a Lustre backend performed significantly (4x) slower than that of a workstation using local fast SSDs for storage. This is due to the fact that file system metadata latency (more specifically, file open) is the determinant factor for performance on HPC, whereas network dominates performance in a typical data centre configuration. It was found by the authors that using either a local in-memory filesystem or a filesystem mounted to a single Lustre file improved performance up to a point where it matched that of a workstation. In addition, use of a cache was found to reduce file system metadata operations, and thus improve overall performance. 
	The issues related to using an in-memory filesystem is that it is limited by the amount of physical memory available, and a job will fail if physical memory runs out. Moreover, there is not persistence or resilience of data as it is not saved to non-volatile storage (disk). Spark fails with medium to large applications due to "lax garbage collection in block and shuffle managers". To resolve these issues, the authors attempted to make all executors share a descriptor pool. This allowed files that were already opened by other executors to be accessed by the executor through the descriptor rather than having to reopen it again. However, the node OS image is subject to number of Inode constraints as well Lustre limits on number of open files for a given job. Due to Inode constraints, a livelock may occur when descriptor pool is at capacity and an executor is attempting to open a file. The authors also created mounted a local filesystem backed by a Lustre file (lustremount) to address the issues related to an in-memory file system \note{requires admin privilege}. It was found that the use of an in-memory file system improved performance by 7.7x and the lustremount improved performance by 6.6x. It was also found that implementation of the filepool improved performance.
	Another issue observed by the authors is that of poor block management as a result of memory constraints resulting in a significant amount of vertical movement. When available memory is exhausted, the least recently used block is evicted by the block-manager. When a call is made to this removed block, it will need to be recomputed, and in the process blocks required for recomputation may also be evicted and necessitate recomputation of those blocks, thus leading to a chain of eviction and recomputation. To resolve this, marking intermediate RDDs as persistent, such that their results are saved to storage instead of requiring recomputation, may be applied.
	\note{they didn't look at improving initial application I/O, though believe that it could be improved with lustremount and filepool. }
	\note{for each partition, a shuffle file is created and written to as many times as there are partitions. An index file is also created which contains shuffle file locations and offsets. Metadata access is therefore O(partitions2)}
	
	\subsection{tachyon paper}
	I/O is a known bottleneck in Spark that is exacerbated in HPC systems in which data is stored in a separate cluster outside compute nodes. One possible solution used to mitigate the effects of the I/O bottleneck is to store data in an in-memory filesystem rather than on disk. Alluxio and Triple-H are two known examples of such filesystems.
	
	Alluxio, formerly known as Tachyon, is an in-memory file system designed to improve read and write performance without impacting fault tolerance of the system. As write performance is impeded by the need to replicate data, Tachyon leverages the concept of lineage to recompute the output if data is lost. Tachyon exhibits 110x faster write throughput than in-memory HDFS and 4.4x faster end-to-end latency. 
	\subsection{triple-H}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	%%%In other words, such frameworks would try to limit data movement, as network cables are expected to have low bandwidth and high latency. In addition, each node in a commodity cluster is expected to have access to its own local storage homogenous to that of the rest of the cluster, thus replication is necessary to ensure data availability. This is, however, not necessarily the case for computing grids. Computing grids may have access to Infinibands, which provide high bandwidth and low latency. Moreover, grids may use heterogenous storage in addition to a parallel file system such as Lustre. As a result, it may, in some cases, be faster to transfer data over the network than to access the nearest copy of the data. As well, data replication may burden a system with centralized storage as multiple copies of the data would be created, taking a significant amount of space.%%%
	
	
	
	%%%Big Data frameworks were not designed with high-performance computing in mind. Big Data filesystems such as were intended to run in data centers with homogenous storage (ex. HDDs, SDDs. etc,.). In such infrastructures, data movement is expensive. To counteract this, strategies were employed to avoid moving data around (data locality). In contrast, HPC clusters use heterogenous storage (ex. a combination of HDD, SSD, Ram Disk and Parallel file systems) with compute nodes connected by high-performance interconnects. HPC nodes may therefore not all be equipped with the same type of storage devices, some of which may be faster than others. Unlike data centers, HPC compute nodes do not have local storage. Moreover, the high-performance interconnects ensure low latency and high throughput of the system. In some instances, it may be even more performant to move data around rather than ensuring data locality. When a single-node HPC cluster using the Luster parallel file system was compared to a workstation with local SSD, it was found that Spark performed about 4x slower on the HPC cluster \cite{Chaimov:2016}.  As HDFS is not conscientious of the underlying infrastructure and expects a data center-type infrastructure, it cannot achieve its full performance potential. %%%
	
	%%%\section{Spark in the cloud}
	
	
	
	
	
	
	
	%%%Using Spark against a typical HPC parallel file system such a Luster has been found to impede scalability. This was found to be due the file system metadata latency, which limits Spark scalability to \textit{O}(10\textsuperscript{2}) cores \cite{Chaimov:2016}.
	
	

	%%%Compute Grids, interconnected networks of computing resources, are heavily used in scientific research for the processing of data. Due to their use and importance to the scientific community, it is necessary that big data frameworks used for processing neuroimaging data are compatible with them. Compute grids typically utilize heterogenous storage, such as a mixture of SSD, HDD, RAM Disk and parallel file systems (e.g. Luster). However, big data frameworks, such as Spark are designed to work efficiently with homogenous storage. Such differences result in reduced performance and inefficient storage use when big data frameworks are used on HPC clusters. This chapter will review the difference techniques applied to big data frameworks as well as HPC and grids to improve collocation of both services.%%%
	
	%%%\subsection{Anatomy of Grid storage}
	%%%Typical grids consist of a cluster of data nodes possibly connected to local storage (e.g. RAM Disk, HDD, SDD), but is also connected through a network to another cluster representing a parallel file system such as Lustre. RAM Disk is the fastest storage as all data is located in memory, whereas SSD provides slightly slower, persistent storage, but is a good alternative for big data. HDDs are the least performant form of local storage.%%% 
	%%%Lustre is a stateful, object-based parallel file system. It consists of three components: 1) the meta data server (MDS), 2) the Object Storage Server (OSS) and 3) the Luster Network (LNET), which enables clients to communicate with each other.%%%

	
	
	
	In an attempt to improve performance of HDFS on HPC clusters, Triple-H was created.
	\section{Why is use of such systems important}
	\section{Scheduling}
		\subsection{HPC schedulers: SLURM/QSUB}
		\subsection{Big Data schedulers: YARN/MESOS}
		\subsection{Multi-level scheduling w/ BigData schedulers}
\chapter{Current status of Big Data frameworks in neuroscience}
	\section{Ariel's paper}
	\section{Thunder}
\chapter{Conclusion}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Body of Thesis goes here.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{bibliography}
\bibliographystyle{ieeetr}
%\bibliography{abbr,chalin,common,larch,tn}  %place your .bib files here
%\bibliographystyle{alpha}                   %the bibliography style to use

\end{document}
