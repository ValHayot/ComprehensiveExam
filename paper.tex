\documentclass{report}                                                           
        
\usepackage{titlesec}
\titleformat{\chapter}
  {\normalfont\LARGE\bfseries}{\thechapter}{1em}{}
\titlespacing*{\chapter}{0pt}{3.5ex plus 1ex minus .2ex}{2.3ex plus .2ex}
\usepackage[left=1in, right=1in, top=1in, bottom=1in]{geometry}
\usepackage{hyperref} 
\usepackage{xcolor} 
\usepackage{ulem}                      
                                                                                 
\newcommand{\note}[1]{\textcolor{blue}{\textit{note}: #1}}                       
\newcommand{\tristan}[1]{\textcolor{red}{TG: #1}}                                
\newcommand{\weird}[1]{\uwave{#1}}  


\begin{document} 
    \title{Pipeline systems and infrastructure for the efficient
            and open processing of Big neuroimaging Data} 
    \author{Valerie Hayot-Sasson}
    \maketitle 
    
    \begin{abstract} Text of abstract.  
    \end{abstract} 
    \tableofcontents
    \chapter{Introduction}
        In recent years, the volume of neuroimaging data acquired has exceeded
        both the storage and computation capacity of a standard research 
        lab workstation. With the advancement of data sharing technologies, this 
        data has been made widely available, \weird{with the only factor impeding
        research on this data being access to infrastructure and software that 
        enables efficient processing on such infrastructure}. Many research labs 
        do have access to high performance computing (HPC) clusters, however, 
        without efficient software to process this data, processing can take an 
        excessive amount of time. Ensuring efficient processing of data is 
        complex and likely beyond the knowledge of an average neuroimaging 
        researcher \tristan{Be careful with 'knowledge', some neuroimaging researchers are very compute-savvy}. As a result, frameworks for the efficient processing of 
        neuroimaging data need to be developed. \tristan{Add a sentence to transition from 'frameworks for efficient processing' to 'workflow engines'}
        
        Many neuroimaging workflow engines currently exist. Examples include
        Nipype, Pipeline System for Octave and Matlab (PSOM), LONI, SPM, FastR,
        Automated Analysis (AA) and Pydpiper\note{add citations here}. These 
        workflow engines aim to satisfy four criteria: 1) straightforward \tristan{easy? facilitated?} 
        workflow composition, 2) performance, 3) portability of the workflows 
        and 4) reproducibility of the analyses. While these workflow engines
        do tackle performance, it is mainly limited to minimizing computation
        time and does not consider data transfer times -- which are typically
        costly in Big Data settings. Therefore, these engines need to be adapted
        for the processing of neuroimaging data.

        Big Data frameworks differ from those available in neuroimaging as their
        focus is mainly on workflow composition and performance. Big Data 
        frameworks such as Apache Spark and Dask \tristan{Add references} both aim to be easy-to-use,
        applicable to a wide variety of analyses and improve performance in the 
        processing of Big Data. These types of systems were primarily designed 
        to be efficient on commodity infrastructure in which transferring data
        over a network would be extremely costly.

        Currently, there exists no Big Data framework which is adapted for the
        processing of neuroimaging data. However, tools such as the Thunder 
        Project \note{add citation} allow for the processing of time-series
        neuroimaging data using Spark as the backend framework. \weird{Tools such as 
        these} provide built-in functions to process the data. For users to 
        extend functionality, it is necessary for them to understand the backend
        framework. Other scientific workflow engines, such as Toil and Pegasus
        \note{add citations}, have incorporated features that enable more 
        performant workflow executions for scientific Big Data. They also enable
        the integration of Big Data frameworks as sub-pipelines within their 
        workflows. Despite integration with Big Data frameworks, it is not
        possible to create Big Data workflows using their API exclusively.

        A big limitation to adapting Big Data frameworks for neuroimaging 
        workflows is that these frameworks are tightly-coupled with the 
        infrastructure they were designed for -- commodity clusters. While
        it is possible for some research labs to be using either a local cluster
        or the cloud for their processing, many may only have access to HPC
        clusters. As a result of this, most available scientific workflow 
        frameworks have been designed for processing on HPC clusters. The 
        set-up of an HPC cluster is quite different from that of a commodity 
        cluster, and therefore, in order to effectively use Big Data frameworks
        for scientific computing, it is necessary to adapt the infrastructure 
        for processing on HPC.

        \tristan{Add a sentence to present the goal of the report.}
        In section~\ref{datasets}, we will briefly introduce the two 
        types of datasets that are giving rise to the neuroimaging big 
        data. Section~\ref{platforms} will provide a brief introduction 
        to the data-sharing and processing platforms available to 
        researchers. We will conclude our 
        introduction on the differences between commodity and HPC infrastructure
        in section~\ref{infrastructure}.

        As we wish to evaluate and compare scientific workflow frameworks with
        Big Data frameworks for the processing of neuroimaging big data, we will
        break down our analyses by the four principal features necessary for
        effective neuroimaging workflow engines. The workflow engines discussed
        will include SPM, LONI, Nipype, PSOM, FastR, AA, Pydpiper 
        for neuroimaging; Taverna, Pegasus, Make, OpenMOLE, Nextflow, Galaxy, 
        Toil for other scientific workflows; and MapReduce, Spark and Dask for
        Big Data Frameworks. Chapter \ref{workcomp} will evaluate these 
        workflows on the basis of workflow composition: we will % 'in other words' is on my list of expressions to avoid
        look at the programming languages necessary to construct these workflows
        , how easy is it to add new tools and will these workflows be understood
        by the community. In Chapter \ref{performance}, we will proceed to 
        investigate the strategies employed by these frameworks to improve the 
        performance of their workflows. Chapter \ref{portability} will discuss
        the portability of the workflows written using these frameworks. 
        Workflow reproducibility will be then discussed in Chapter 
        \ref{reproducibility}.
        

        \section{Big Neuroimaging Datasets}\label{datasets}

            As a result of the rise in data sharing in neuroscience, neuroimaging
            Big Data has come to fruition. The field of neuroscience has now
            reached a bottleneck where there exists sufficient data to make
            impactful studies, but the software to process these datasets have 
            not evolved with the size of the data.

            Neuroimaging big data comes in two formats: 1) large images and 2) 
            large datasets. Large images consist of singular images ranging 
            from 100s of gigabytes to terabytes in size. A well-known example of 
            such an image is the BigBrain \note{cite here}, a histological image 
            of the brain of a healthy 69 year-old man. At its highest resolution
            , it is 1x1x20$\mu$m or 1TB in
            size. Other examples of large images can be found in electron 
            microscopy (EM), polarized light imaging (PLI) and micro computational
            tomography (microCT) \note{need to do more research here.cite}. 
            Images at 
            such high resolution are important to researchers as they provide 
            insights into aspects not otherwise detectible in images at lower 
            resolutions. However, due to their size and lack of resources 
            available to process such images, research on this data remains 
            limited.

            In contrast, large datasets consist of many images acquired from
            multiple subjects, which are too large to be processed in their
            entirety. Notable examples of these datasets include the 
            Alzheimer's Disease Neuroimaging Initiative (ADNI), OpenfMRI, 
            the Autism Brain Imaging Data Exchange (ABIDE), the Consortium for
            Reliability and Reproducibility (CoRR), the Human Connectome Project
            (HCP) and the UK Biobank, with the later
            two being flagship recent examples of large datasets in neuroimaging.

            The ADNI intiative is a collection of images (structural and 
            functional MRI, PET), urine,
            serum, cerebrospinal fluid biomarkers, as assessments (clinical and
            neuropsychometric) of 800 elderly subjects, which of whom 200 where
            healthy, 400 exhibited mild cognitive impairment and the remaining
            200 showed signs of Alzheimers. Imaging data is available both 
            pre- and post-processed form.

            OpenfMRI is a public repository originally conceived for the sharing
            of task-based fMRI data. Over time, it has grown to include EEG, MEG
            , resting-state fMRI (R-fMRI) and diffusion fMRI. Data made 
            available through
            OpenfMRI are obtained from both healthy and clinical individuals.
            Raw data must be made available for all datasets published to 
            OpenfMRI, however, preprocessed data may also accompany the raw data
            . At present, OpenfMRI houses data of a total 3372 subjects
            originating from 95 studies \note{obtained from website}.

            ABIDE was conceived to promote the advancement of knowledge 
            of Autism Spectrum Disorders (ASD). % inverted the sentence to keep symmetry with the other paragraphs
            ABIDE is an open repository of 
            preprocessed data obtained from 1112 R-fMRI datasets.
            In addition to R-fMRI, datasets also contain the corresponding 
            structural MRI and phenotypic data. As data made available in ABIDE
            is preprocessed, standardized pipelines were developed for the 
            preprocessing of the images. Popular neuroimaging processing tools
            such as CIVET, ANTs, and FreeSurfer \note{cite} were used for the 
            preprocessing of structural data, and several other pipelines were
            used for the processing of functional data.

            The CoRR \tristan{expand acronym} was developed as an initiative to study and improve the 
            reliability and reproducibility of functional connectomics. The 
            data made available by the Consortium consists of test-retest data 
            obtained from 33 datasets. The repository contains data from 1629 
            subjects, in which anatomical, R-fMRI, dMRI, CBF and ASL scans are 
            available \note{obtained from website}.


            The HCP initiative aimed to characterize the human connectome of 
            1200 healthy adults, ranging from 22 to 35 years of age, by studying
            twins and their non-twin siblings. Data collected for this initiave
            includes structural MRI, R-fMRI, task fMRI and dMRI. MRI data was
            collected using a 3T scanner for all participants, with a 200 
            participants additionally undergoing scans in a higher-resolution 7T
            scanner. MEG and EEG data is available for a subset of the subjects. 
            In addition to imaging data, behavioural and genetic data were also 
            acquired. To facilitate analysis for research groups, HCP made 
            available the raw data, minimally pre-processed data and processed 
            data. Pipelines were developed by the HCP for pre- and post-
            processing. As in ABIDE, these pipelines reused many commonly-used
            neuroimaging tools such as FreeSurfer and FSL for structural data
            and FSL and other tools for functional data. The total amount of
            data made available by the HCP reaches up to nearly 1 Petabyte.


            The UK Biobank is an initiative aimed at the detection of pre-
            symptomatic markers of diseases occurring in middle-aged to elderly 
            populations. The study will gather data from questionnaires, 
            physical and cognitive measures and biological samples of 500,000
            individuals. 100,000 of these individuals will additionally be 
            subjected to imaging studies. Imaging modalities used include
            structural MRI, fMRI and dMRI of the brain heart and body; low-dose
            X-ray bone and joint scans; and ultrasound of the carotid arteries.
            As with other initiatives, the UK Biobank makes available both 
            raw and processed data. This data is expected to exceed 0.2 
            Petabytes. Since processed data is provided, the UK Biobank makes 
            available it own pipelines based on tools in FSL, but also plans to
            adapt HCP pipelines from cortical surface extraction in their images
            .

            All these initiative stress the importance of data-sharing and
            large-scale studies for the advancement of science. Many researchers
            are limited to the processing of subsets of the data available as 
            they lack compute and storage resources to process the data in its
            entirety. It is also important to note that many of the pipelines 
            developed by these initiatives made use of popular neuroimaging 
            toolboxes, such as FSL, FreeSurfer, ANTS, CIVET, rather that
            creating their own. Therefore, there is a great need for Big Data
            frameworks that can leverage existing tools for the efficient 
            processing of large-scale datasets. With the availability of such 
            tools, many more research labs will be able to perform
            higher-impact studies \note{reword}.


        \section{Open-science Platforms}\label{platforms}
            Efficient processing of workflows are only but a small portion of 
            the Big Data issue in neuroscience. % nice transition, this section gives more context for your study
            Sharing and storing of datasets
            and tools, secure access to and configuration of multiple computing 
            sites and provenance capture of all activities must be managed by 
            the researcher. This puts a heavy burden on researchers as they 
            require some knowledge of all these components in order to manage it
            correctly. These platforms also play a vital role in promoting 
            research through the sharing of knowledge. Users can choose to share
            their data and pipelines with others, reducing the need for others 
            to collect their own data or to develop their own workflows. It 
            also promotes robustness of results as the data and workflows will 
            be shared and tested by multiple users.

            Access to HPC resources may not always be straight-forward as 
            configuration must be adapted to the execution site. This can 
            range from scheduling policies, to available file systems, to 
            environment and available libraries. Platforms relieve the user
            from having to know site-specific details by providing seamless
            execution on any cluster the Platform has knowledge of.

            Examples of popular neuroinformatics open-science platforms include 
            CBRAIN\note{cite paper}, OpenNeuro \note{cite poster} and Neurodata
            \note{cite prepring}. OpenNeuro and NeuroData primarily focus on
            cloud deployement whereas CBRAIN is configured for the deployment
            of workflows on HPC resources. All three of these systems allow 
            users to configure, manage, share and execute their workflows with 
            the help of a web-based GUI. Integration with compute resources is
            entirely abstracted by the platforms. These platforms additionally
            provide tools to visualize their data.

            As neuroimaging workflow engines are typically best suited for 
            compute-intensive workflows, the workflows accessible in these 
            platforms are not expected to be as suitable to process the 
            volume of data made accessible by these platforms. Since these 
            platforms make various infrastructures available to researchers, 
            workflow engines must be adapted to efficiently execute on the 
            infrastructure made available.
            
        \section{Infrastructure}\label{infrastructure}
            HPC infrastructure was designed during a time when compute resources
            were scarce. As a result, \tristan{I disagree with 'As a result' and I don't particularly agree with the previous sentence: even if
            resources aren't scarce, they are good reasons to optimize resource managers: energy consumption, application makespan, fairness, etc. Also,
            I'm not sure if resources are less scarce now than they used to be: with the rise of computational sciences, almost every researcher now
            needs a cluster, and Compute Canada (but also other infrastructures) has been very busy.}
            resource managers for these systems employ
            strategies to efficiently use the compute resources available for 
            processing. However, with the advent of big data, there has been a 
            shift from compute-intensive to data-intensive workloads. That is, 
            with the accumulation of large datasets, data transfers have begun 
            to monopolize processing time, and with it software has been 
            designed to minimize data transfers.

            We define HPC systems as those that employ shared parallel 
            filesystem such as Lustre \tristan{and data-unaware schedulers?}.
            \tristan{You should say that this definition is 'by contrast to big data platforms' to explain where it's coming from.}
            This shared filesystem is the slowest of
            all storage devices as it is shared amongst all users of the system.
            Compute nodes in an HPC system may or may not contain any local 
            storage and network interconnects between nodes are typically fast.
            Resources managers used in HPC infrastructures typically employ 
            batch scheduling and execute heterogenous workflows. 
            Examples of schedulers used in HPC clusters 
            include SLURM, PBS, SGE and HTCondor \note{cite here}.

            Commodity infrastructure \tristan{Call it 'Big Data infrastructure?'} differs from HPC infrastructures in that
            it uses less-costly hardware. It follows a shared-nothing paradigm
            where the storage is not shared between nodes. That is, all compute
            nodes are responsible for storing the data as well. File systems
            used in these clusters are distributed filesystems, such as the
            Hadoop Distributed File System (HDFS) and Alluxio \note{cite}.
            Fault-tolerance of the data is achieved through replication across
            nodes. As transferring data across the network is typically costly 
            in such infrastructures, frameworks designed to run on commodity 
            infrastructure attempt to limit the transfer of data across nodes. 
            Resource managers created specifically for these systems
            typically expect dedicated infrastructure, as is the case with 
            Yet Another Resource Negotiator (YARN), but can also execute 
            heterogeneous workflows, as can be seen with Mesos \note{cite}.

            Commodity clusters may also come in the form of cloud infrastructure.
            While the hardware employed is typically the same as other
            commodity clusters, the software used varies. Cloud infrastructures
            have the added benefit of virtualization, allowing users to 
            configure the infrastructure and software requirements to meet their
            needs. Cloud infrastructure also enables researchers to request
            dedicated resources without having to maintain these resources. One
            of the biggest limiting factors of cloud resources is the cost 
            associated with usage.

            Researchers typically have access to HPC infrastructure, but may
            also have access to commodity infrastructure in the format of a 
            local cluster or cloud. Although both neuroimaging frameworks and
            big data frameworks can be made to execute in either environments,
            the performance improvements that they bring are very much 
            dependent on infrastructure that they are executed on. For instance,
            neuroimaging workflows typically read and write to a shared file
            system at each workflow step. This is less of an issue in HPC 
            infrastructure as network bandwidth is typically better than on 
            commodity infrastructure. In contrasts, Big Data framework typically
            rely on there being some form of local storage. Employing a Big Data
            framework on HPC infrastructure may in turn result in less to no 
            data locality -- a strategy employed by Big Data frameworks to 
            minimize data movement across the network. Due to the variability in
            infrastructure available to researchers, a workflow framework good
            for neuroscience must be well adapted to handle these differences.

    \chapter{Workflow composition}\label{workcomp}
        \section{Programming language}
        \section{Modularity}
        \section{Workflow sharing}
    \chapter{Performance}\label{performance}
        \section{Filesystems}
        \section{Scheduling}
        \section{Performance enhancement strategies}
    \chapter{Portability}\label{portability}
        \section{Containers}
    \chapter{Reproducibility}\label{reproducibility}
        \section{Provenance Tracking}
        \section{containerization}
    \chapter{Discussion/Conclusion}


    \addcontentsline{toc}
        {chapter}{Bibliography} 
        \bibliography{bibliography}
        \bibliographystyle{ieeetr}
\end{document}
