\documentclass{report}                                                           
        
\usepackage{titlesec}
\titleformat{\chapter}
  {\normalfont\LARGE\bfseries}{\thechapter}{1em}{}
\titlespacing*{\chapter}{0pt}{3.5ex plus 1ex minus .2ex}{2.3ex plus .2ex}
\usepackage[left=1in, right=1in, top=1in, bottom=1in]{geometry}
\usepackage{hyperref} 
\usepackage{xcolor} 
\usepackage{ulem}                      
                                                                                 
\newcommand{\note}[1]{\textcolor{blue}{\textit{note}: #1}}                       
\newcommand{\tristan}[1]{\textcolor{red}{TG: #1}}                                
\newcommand{\weird}[1]{\uwave{#1}}  


\begin{document} 
    \title{Pipeline systems and infrastructure for the efficient
            and open processing of Big neuroimaging Data} 
    \author{Valerie Hayot-Sasson}
    \maketitle 
    
    \begin{abstract} Text of abstract.  
    \end{abstract} 
    \tableofcontents
    \chapter{Introduction}
        In recent years, the volume of neuroimaging data acquired has exceeded
        both the storage and computation capacity of a standard research 
        lab workstation. With the advancement of data sharing technologies, this 
        data has been made widely available, \weird{with the only factor impeding
        research on this data being access to infrastructure and software that 
        enables efficient processing on such infrastructure}. Many research labs 
        do have access to high performance computing (HPC) clusters, however, 
        without efficient software to process this data, processing can take an 
        excessive amount of time. Ensuring efficient processing of data is 
        complex and likely beyond the knowledge of an average neuroimaging 
        researcher \tristan{Be careful with 'knowledge', some neuroimaging researchers are very compute-savvy}. As a result, frameworks for the efficient processing of 
        neuroimaging data need to be developed. \tristan{Add a sentence to transition from 'frameworks for efficient processing' to 'workflow engines'}
        
        Many neuroimaging workflow engines currently exist. Examples include
        Nipype, Pipeline System for Octave and Matlab (PSOM), LONI, SPM, FastR,
        Automated Analysis (AA) and Pydpiper\note{add citations here}. These 
        workflow engines aim to satisfy four criteria: 1) straightforward \tristan{easy? facilitated?} 
        workflow composition, 2) performance, 3) portability of the workflows 
        and 4) reproducibility of the analyses. While these workflow engines
        do tackle performance, it is mainly limited to minimizing computation
        time and does not consider data transfer times -- which are typically
        costly in Big Data settings. Therefore, these engines need to be adapted
        for the processing of neuroimaging data.

        Big Data frameworks differ from those available in neuroimaging as their
        focus is mainly on workflow composition and performance. Big Data 
        frameworks such as Apache Spark and Dask \tristan{Add references} both aim to be easy-to-use,
        applicable to a wide variety of analyses and improve performance in the 
        processing of Big Data. These types of systems were primarily designed 
        to be efficient on commodity infrastructure in which transferring data
        over a network would be extremely costly.

        Currently, there exists no Big Data framework which is adapted for the
        processing of neuroimaging data. However, tools such as the Thunder 
        Project \note{add citation} allow for the processing of time-series
        neuroimaging data using Spark as the backend framework. \weird{Tools such as 
        these} provide built-in functions to process the data. For users to 
        extend functionality, it is necessary for them to understand the backend
        framework. Other scientific workflow engines, such as Toil and Pegasus
        \note{add citations}, have incorporated features that enable more 
        performant workflow executions for scientific Big Data. They also enable
        the integration of Big Data frameworks as sub-pipelines within their 
        workflows. Despite integration with Big Data frameworks, it is not
        possible to create Big Data workflows using their API exclusively.

        A big limitation to adapting Big Data frameworks for neuroimaging 
        workflows is that these frameworks are tightly-coupled with the 
        infrastructure they were designed for -- commodity clusters. While
        it is possible for some research labs to be using either a local cluster
        or the cloud for their processing, many may only have access to HPC
        clusters. As a result of this, most available scientific workflow 
        frameworks have been designed for processing on HPC clusters. The 
        set-up of an HPC cluster is quite different from that of a commodity 
        cluster, and therefore, in order to effectively use Big Data frameworks
        for scientific computing, it is necessary to adapt the infrastructure 
        for processing on HPC.

        \tristan{Add a sentence to present the goal of the report.}
        In section~\ref{datasets}, we will briefly introduce the two 
        types of datasets that are giving rise to the neuroimaging big 
        data. Section~\ref{platforms} will provide a brief introduction 
        to the data-sharing and processing platforms available to 
        researchers. We will conclude our 
        introduction on the differences between commodity and HPC infrastructure
        in section~\ref{infrastructure}.

        As we wish to evaluate and compare scientific workflow frameworks with
        Big Data frameworks for the processing of neuroimaging big data, we will
        break down our analyses by the four principal features necessary for
        effective neuroimaging workflow engines. The workflow engines discussed
        will include SPM, LONI, Nipype, PSOM, FastR, AA, Pydpiper 
        for neuroimaging; Taverna, Pegasus, Make, OpenMOLE, Nextflow, Galaxy, 
        Toil for other scientific workflows; and MapReduce, Spark and Dask for
        Big Data Frameworks. Chapter \ref{workcomp} will evaluate these 
        workflows on the basis of workflow composition: we will % 'in other words' is on my list of expressions to avoid
        look at the programming languages necessary to construct these workflows
        , how easy is it to add new tools and will these workflows be understood
        by the community. In Chapter \ref{performance}, we will proceed to 
        investigate the strategies employed by these frameworks to improve the 
        performance of their workflows. Chapter \ref{portability} will discuss
        the portability of the workflows written using these frameworks. 
        Workflow reproducibility will be then discussed in Chapter 
        \ref{reproducibility}.
        

        \section{Big Neuroimaging Datasets}\label{datasets}

            As a result of the rise in data sharing in neuroscience, neuroimaging
            Big Data has come to fruition. The field of neuroscience has now
            reached a bottleneck where there exists sufficient data to make
            impactful studies, but the software to process these datasets have 
            not evolved with the size of the data.

            Neuroimaging big data comes in two formats: 1) large images and 2) 
            large datasets. Large images consist of singular images ranging 
            from 100s of gigabytes to terabytes in size. A well-known example of 
            such an image is the BigBrain \note{cite here}, a histological image 
            of the brain of a healthy 69 year-old man. At its highest resolution
            , it is 1x1x20$\mu$m or 1TB in
            size. Other examples of large images can be found in electron 
            microscopy (EM), polarized light imaging (PLI) and micro computational
            tomography (microCT) \note{need to do more research here.cite}. 
            Images at 
            such high resolution are important to researchers as they provide 
            insights into aspects not otherwise detectible in images at lower 
            resolutions. However, due to their size and lack of resources 
            available to process such images, research on this data remains 
            limited.

            In contrast, large datasets consist of many images acquired from
            multiple subjects, which are too large to be processed in their
            entirety. Notable examples of these datasets include the 
            Alzheimer's Disease Neuroimaging Initiative (ADNI), OpenfMRI, 
            the Autism Brain Imaging Data Exchange (ABIDE), the Consortium for
            Reliability and Reproducibility (CoRR), the Human Connectome Project
            (HCP) and the UK Biobank, with the later
            two being flagship recent examples of large datasets in neuroimaging.

            The ADNI intiative is a collection of images (structural and 
            functional MRI, PET), urine,
            serum, cerebrospinal fluid biomarkers, as assessments (clinical and
            neuropsychometric) of 800 elderly subjects, which of whom 200 where
            healthy, 400 exhibited mild cognitive impairment and the remaining
            200 showed signs of Alzheimers. Imaging data is available both 
            pre- and post-processed form.

            OpenfMRI is a public repository originally conceived for the sharing
            of task-based fMRI data. Over time, it has grown to include EEG, MEG
            , resting-state fMRI (R-fMRI) and diffusion fMRI. Data made 
            available through
            OpenfMRI are obtained from both healthy and clinical individuals.
            Raw data must be made available for all datasets published to 
            OpenfMRI, however, preprocessed data may also accompany the raw data
            . At present, OpenfMRI houses data of a total 3372 subjects
            originating from 95 studies \note{obtained from website}.

            ABIDE was conceived to promote the advancement of knowledge 
            of Autism Spectrum Disorders (ASD). % inverted the sentence to keep symmetry with the other paragraphs
            ABIDE is an open repository of 
            preprocessed data obtained from 1112 R-fMRI datasets.
            In addition to R-fMRI, datasets also contain the corresponding 
            structural MRI and phenotypic data. As data made available in ABIDE
            is preprocessed, standardized pipelines were developed for the 
            preprocessing of the images. Popular neuroimaging processing tools
            such as CIVET, ANTs, and FreeSurfer \note{cite} were used for the 
            preprocessing of structural data, and several other pipelines were
            used for the processing of functional data.

            The CoRR \tristan{expand acronym} was developed as an initiative to study and improve the 
            reliability and reproducibility of functional connectomics. The 
            data made available by the Consortium consists of test-retest data 
            obtained from 33 datasets. The repository contains data from 1629 
            subjects, in which anatomical, R-fMRI, dMRI, CBF and ASL scans are 
            available \note{obtained from website}.


            The HCP initiative aimed to characterize the human connectome of 
            1200 healthy adults, ranging from 22 to 35 years of age, by studying
            twins and their non-twin siblings. Data collected for this initiave
            includes structural MRI, R-fMRI, task fMRI and dMRI. MRI data was
            collected using a 3T scanner for all participants, with a 200 
            participants additionally undergoing scans in a higher-resolution 7T
            scanner. MEG and EEG data is available for a subset of the subjects. 
            In addition to imaging data, behavioural and genetic data were also 
            acquired. To facilitate analysis for research groups, HCP made 
            available the raw data, minimally pre-processed data and processed 
            data. Pipelines were developed by the HCP for pre- and post-
            processing. As in ABIDE, these pipelines reused many commonly-used
            neuroimaging tools such as FreeSurfer and FSL for structural data
            and FSL and other tools for functional data. The total amount of
            data made available by the HCP reaches up to nearly 1 Petabyte.


            The UK Biobank is an initiative aimed at the detection of pre-
            symptomatic markers of diseases occurring in middle-aged to elderly 
            populations. The study will gather data from questionnaires, 
            physical and cognitive measures and biological samples of 500,000
            individuals. 100,000 of these individuals will additionally be 
            subjected to imaging studies. Imaging modalities used include
            structural MRI, fMRI and dMRI of the brain heart and body; low-dose
            X-ray bone and joint scans; and ultrasound of the carotid arteries.
            As with other initiatives, the UK Biobank makes available both 
            raw and processed data. This data is expected to exceed 0.2 
            Petabytes. Since processed data is provided, the UK Biobank makes 
            available it own pipelines based on tools in FSL, but also plans to
            adapt HCP pipelines from cortical surface extraction in their images
            .

            All these initiative stress the importance of data-sharing and
            large-scale studies for the advancement of science. Many researchers
            are limited to the processing of subsets of the data available as 
            they lack compute and storage resources to process the data in its
            entirety. It is also important to note that many of the pipelines 
            developed by these initiatives made use of popular neuroimaging 
            toolboxes, such as FSL, FreeSurfer, ANTS, CIVET, rather that
            creating their own. Therefore, there is a great need for Big Data
            frameworks that can leverage existing tools for the efficient 
            processing of large-scale datasets. With the availability of such 
            tools, many more research labs will be able to perform
            higher-impact studies \note{reword}.


        \section{Open-science Platforms}\label{platforms}
            Efficient processing of workflows are only but a small portion of 
            the Big Data issue in neuroscience. % nice transition, this section gives more context for your study
            Sharing and storing of datasets
            and tools, secure access to and configuration of multiple computing 
            sites and provenance capture of all activities must be managed by 
            the researcher. This puts a heavy burden on researchers as they 
            require some knowledge of all these components in order to manage it
            correctly. These platforms also play a vital role in promoting 
            research through the sharing of knowledge. Users can choose to share
            their data and pipelines with others, reducing the need for others 
            to collect their own data or to develop their own workflows. It 
            also promotes robustness of results as the data and workflows will 
            be shared and tested by multiple users.

            Access to HPC resources may not always be straight-forward as 
            configuration must be adapted to the execution site. This can 
            range from scheduling policies, to available file systems, to 
            environment and available libraries. Platforms relieve the user
            from having to know site-specific details by providing seamless
            execution on any cluster the Platform has knowledge of.

            Examples of popular neuroinformatics open-science platforms include 
            CBRAIN\note{cite paper}, OpenNeuro \note{cite poster} and Neurodata
            \note{cite prepring}. OpenNeuro and NeuroData primarily focus on
            cloud deployement whereas CBRAIN is configured for the deployment
            of workflows on HPC resources. All three of these systems allow 
            users to configure, manage, share and execute their workflows with 
            the help of a web-based GUI. Integration with compute resources is
            entirely abstracted by the platforms. These platforms additionally
            provide tools to visualize their data.

            As neuroimaging workflow engines are typically best suited for 
            compute-intensive workflows, the workflows accessible in these 
            platforms are not expected to be as suitable to process the 
            volume of data made accessible by these platforms. Since these 
            platforms make various infrastructures available to researchers, 
            workflow engines must be adapted to efficiently execute on the 
            infrastructure made available.
            
        \section{Infrastructure}\label{infrastructure}
            HPC infrastructure was designed during a time when compute resources
            were scarce. As a result, \tristan{I disagree with 'As a result' and I don't particularly agree with the previous sentence: even if
            resources aren't scarce, they are good reasons to optimize resource managers: energy consumption, application makespan, fairness, etc. Also,
            I'm not sure if resources are less scarce now than they used to be: with the rise of computational sciences, almost every researcher now
            needs a cluster, and Compute Canada (but also other infrastructures) has been very busy.}
            resource managers for these systems employ
            strategies to efficiently use the compute resources available for 
            processing. However, with the advent of big data, there has been a 
            shift from compute-intensive to data-intensive workloads. That is, 
            with the accumulation of large datasets, data transfers have begun 
            to monopolize processing time, and with it software has been 
            designed to minimize data transfers.

            We define HPC systems as those that employ shared parallel 
            filesystem such as Lustre \tristan{and data-unaware schedulers?}.
            \tristan{You should say that this definition is 'by contrast to big data platforms' to explain where it's coming from.}
            This shared filesystem is the slowest of
            all storage devices as it is shared amongst all users of the system.
            Compute nodes in an HPC system may or may not contain any local 
            storage and network interconnects between nodes are typically fast.
            Resources managers used in HPC infrastructures typically employ 
            batch scheduling and execute heterogenous workflows. 
            Examples of schedulers used in HPC clusters 
            include SLURM, PBS, SGE and HTCondor \note{cite here}.

            Commodity infrastructure \tristan{Call it 'Big Data infrastructure?'} differs from HPC infrastructures in that
            it uses less-costly hardware. It follows a shared-nothing paradigm
            where the storage is not shared between nodes. That is, all compute
            nodes are responsible for storing the data as well. File systems
            used in these clusters are distributed filesystems, such as the
            Hadoop Distributed File System (HDFS) and Alluxio \note{cite}.
            Fault-tolerance of the data is achieved through replication across
            nodes. As transferring data across the network is typically costly 
            in such infrastructures, frameworks designed to run on commodity 
            infrastructure attempt to limit the transfer of data across nodes. 
            Resource managers created specifically for these systems
            typically expect dedicated infrastructure, as is the case with 
            Yet Another Resource Negotiator (YARN), but can also execute 
            heterogeneous workflows, as can be seen with Mesos \note{cite}.

            Commodity clusters may also come in the form of cloud infrastructure.
            While the hardware employed is typically the same as other
            commodity clusters, the software used varies. Cloud infrastructures
            have the added benefit of virtualization, allowing users to 
            configure the infrastructure and software requirements to meet their
            needs. Cloud infrastructure also enables researchers to request
            dedicated resources without having to maintain these resources. One
            of the biggest limiting factors of cloud resources is the cost 
            associated with usage.

            Researchers typically have access to HPC infrastructure, but may
            also have access to commodity infrastructure in the format of a 
            local cluster or cloud. Although both neuroimaging frameworks and
            big data frameworks can be made to execute in either environments,
            the performance improvements that they bring are very much 
            dependent on infrastructure that they are executed on. For instance,
            neuroimaging workflows typically read and write to a shared file
            system at each workflow step. This is less of an issue in HPC 
            infrastructure as network bandwidth is typically better than on 
            commodity infrastructure. In contrasts, Big Data framework typically
            rely on there being some form of local storage. Employing a Big Data
            framework on HPC infrastructure may in turn result in less to no 
            data locality -- a strategy employed by Big Data frameworks to 
            minimize data movement across the network. Due to the variability in
            infrastructure available to researchers, a workflow framework good
            for neuroscience must be well adapted to handle these differences.

    \chapter{Workflow composition}\label{workcomp}

        Workflows, also known as pipelines, describe the flow of data from 
        activity to another. They are typically represented by a 
        Directed Acyclic Graph (DAG) \tristan{Not necessarily a DAG, see Nipype for instance, it has loops.
        I don't think you want to enter in a taxonomy of languages though, just say 'graph'}. Workflow activites, also referred to as 
        workflows stages, encapsulate a tool \tristan{Mention 'interface' here as an intro to sub-sec 2}. Tools may either take the form of 
        a command-line tool or a custom script \tristan{A script is a command-line tool. I think tools may also be web services (Taverna) or just 
        software libraries (Dask)}. The engines determine which
        workflow activity can be staged by determining if the stage dependencies
        have been satisfied.

        In order for a workflow engine to be successful, it must be simple to 
        compose workflows using that framework. New researchers should be able 
        to acquire the skills to compose worfklows in that framework without
        investing a significant amount of time in understanding it. Reuse of
        existing tools is also common occurrence in neuroinformatics and 
        workflow engines need to accomodate this by undertaking a modular 
        approach and being easily extensible. With the sharing of data also 
        comes the sharing of workflows. For labs to share their workflows with
        the community at large, these workflows must be easily understood by the
        community. \tristan{Perhaps mention that this also relates to the sharing
        of reproducible results}.

        In this Chapter, we will compare and contrast the workflow composition 
        of seven neuroimaging frameworks (SPM, LONI, Nipype, PSOM, FastR, AA and
        Pydpiper), seven scientific workflow engines stemming from other fields
        (Taverna, Pegasus, Make, OpenMOLE, Nextflow, Galaxy, Toil), and three
        BigData frameworks (MapReduce, Spark, Dask). We will use three axes to
        evaluate workflow composition. Section~\ref{lang} will compare the 
        framework on the basis of programming language used, whereas 
        Section~\ref{mod} will look at how easy it is to integrate existing 
        tools into the workflows. Section~\ref{sharing} will evaluate the ease
        at which their corresponding workflows can be shared.

        \section{Workflow language}\label{lang}
            The choice of workflow language (e.g. GUI, scripting language,
            programming language) greatly
            affects how easily it will get picked up by the community. Complex
            languages may mean greater efficiency of the engines
            , but also result in extensive time spent on learning and developing
            the pipelines. Moreover, some researchers may not be well-versed in
            programming and may require very easy-to-use engines.

            To address this issue, some workflow engines have taken the route
            of employing a graphical user interface (GUI). This makes workflow 
            composition accessible to all researchers, including those with
            little to no programming background. The caveat to employing a 
            workflow engine with a GUI interface for workflow composition is
            that users are greatly limited by the tasks they can perform. 
            Workflow engines that employ a worfklow composition GUI include
            SPM, Taverna, OpenMOLE, LONI and Galaxy. \note{double-check that 
            none have a programming API.Loni does..i think SPM does too}
            \tristan{My conclusion on GUIs is that they are suitable only
            for very simple pipelines. Perhaps the intro could give an idea
            of the types of pipelines encountered, showing a simple one and
            a more complex one. No way fmriprep is described in a GUI.}

            As more experienced programmers may feel limited by a GUI, workflow
            engines have decided to provide users with the ability to describe
            their workflows in more flexible formats. Even those with GUIs for 
            workflow composition, such as LONI and OpenMOLE, allow more 
            experienced users to script their workflow. Six different 
            programming
            languages are used for describing scientific workflows: 1) Python, 
            2) MATLAB and Octave \tristan{I think the official name is GNU/Octave}, 3) Shell \tristan{scripting languages}, 4) XML, 6) Common Workflow Language 
            (CWL) and 6) Domain Specific Language (DSL). \tristan{I don't see why CWL and XML don't qualify as DSL} Workflows engines that 
            employ usage of Python include: Nipype, FastR, Pydpiper, Pegasus
            and Toil \tristan{Dask? Spark?}.
            Python is an excellent choice as a programming language for 
            scientific workflows, particularly neuroinformatics, due to its 
            widespread use within the community. Other advantages of 
            programming in Python including a wide array of available libraries,
            including neuroimaging-specific ones such as Nilearn and Nibabel \tristan{mention scipy, sci-kit learn, etc}
            \note{cite}, and an lower learning curve compared to other 
            programming languages. Additionally, Python is platform independent
            meaning it can be installed on any system. In terms of BigData 
            frameworks that employ Python, Dask is exclusively written in Python
            , whereas Hadoop MapReduce and Spark both have interfaces to Python
            while being written in Java and Scala, respectively.

            Another widely-used high-level programming language in the 
            neuroinformatics community is MATLAB/Octave. Workflow engines which
            use this language include: PSOM, SPM, and Automatic analysis. A 
            downfall to imposing MATLAB as programming language is that the 
            it is proprietary, thus imposing a fee on 
            anyone wishes to program in that language \tristan{You should also mention the
            advantage of Matlab: easy to learn, integrated ecosystem, 
            performance, see 
            \url{https://www.mathworks.com/products/matlab/matlab-vs-python.html}}. 
            To offset this, PSOM ensure compatibility with the GNU 
            Octave programming language, thus 
            removing the requirement to purchase software for workflow 
            composition. Similarly to Python, MATLAB and Octave are 
            high-level platform-independent languages. MATLAB, in particular,
            also provides a wide array of libraries, however, the software must
            be purchased in order to access them.

            \tristan{This paragraph should start with ``Shell scripting ...". The structuration of this section by language
            is great, but the writing of this paragram doesn't put it forward.}Scientific workflows are typically composed in UNIX-based 
            environments. These environments typically have Make installed by
            default eliminating the need for users to install it manually.
            Furthermore, all users who use Linux environments will typically 
            need to write some form of Shell script. This is the basis to use 
            workflow engines, such as Make, as scientific workflow engines. 
            As Make is exclusive to UNIX environments, it is not 
            platform-independent. However, Make has the flexibility of being
            easily extensible and can interface directly with any command-line
            tool. Shell scripting, nevertheless, is generally not as intuitive
            as programming languages like Python and MATLAB.

            Composing workflows in XML is another option provided by some 
            workflow engines such as Pegasus and LONI. However, the use of a 
            markup language removes some of the flexibility. \tristan{Before you explain the downside, 
            you should say what are the advantages: mainly language-independent parsers and validation.} Users wanting to 
            include 
            custom scripts as workflow activities must do so by creating a 
            command-line interface for their custom script. Pegasus, however,
            does provide many interfaces to describe the pipeline in including
            Java, Python and Perl.

            Another alternative is to compose workflows using Common Workflow
            Language (CWL). This can be seen in workflow engines like Toil. 
            CWL is not a programming language, but a specification for workflows
            \tristan{It is a YAML-based DSL for workflows. Just like Pegasus' DAX is an XML-based DSL}. Files can either be written in JSON or YAML formats. CWL is 
            based on workflow engines like Make \tristan{? I don't think CWL relies on make, does it?}, but differs in that tasks
            are isolated and inputs and outputs must be explicit \tristan{inputs and outputs are somewhat explicit in a Makefile}. Similarly to 
            make, CWL is only available on UNIX platforms \tristan{strange, the CWL interpreter is in Python so I'd expect it to run anywhere}. \note{cite cwl 
            website}

            Domain Specific Languages (DSL) are also used to compose workflows.
            DSLs are programming languages that have been adapted to specific
            domains. Workflow engine examples include OpenMOLE, which was 
            derived from Scala, and Netflow \tristan{Nextflow?}, which is derived from the Apache 
            Groovy programming language. While both are written on top of the 
            Java platform, Nextflow is only compatible with UNIX-based \tristan{How about using Linux everywhere instead of UNIX-based?} 
            environments. These programming languages are not otherwise commonly
            used in neuroimaging, meaning that users must learn specific 
            languages that would not otherwise be used in their field.

            With the exeption of LONI, neuroimaging workflows tend to stick to
            commonly used programming languages such as Python and MATLAB/Octave
            . This is likely due to the fact that the learning curve to adapt to
            these languages is relatively low and users can harness the 
            flexibility of these languages. 
            
            MapReduce, Spark and Dask all provide interfaces to Python, making 
            them good candidates for neuroimaging workflow engines. In addition
            Spark also interfaces with R, another programming language commonly
            used in neuroimaging.

            \subsection{Data structures}
                \subsubsection{Scientific workflows}
                \subsubsection{Big Data: MapReduce}
            

            \tristan{So this is now about data structures, not just language.
            This is really excellent, but this should perhaps go into a 
            specific sub-section. I think you could mention that 
            scientific workflow engines traditionally rely on files 
            only, and simple types, but that big data engines have 
            invested a lot in data structures, starting from key-value 
            pair representations in MR to RDD and Dask arrays. Spark is 
            actually built around RDDs, this is salient-enough to be 
            discussed a bit more. It actually makes a lot of sense that 
            data-focused engines are built around data structures 
            instead of control constructs. The ACM Com Spark paper makes it clear
            that unifying data structures is key to performance and interoperability, 
            because it allows data to remain in-memory instead of having to go back
            and forth to different environments. I would only mention linealg 
            in the description of Dask arrays. So, how about a 
            sub-section 'data structures' with 4 paragraphs: (1) 
            scientific and neuroimaging engines, (2) Big-Data, MR, (3) 
            Spark's RDDs and DataFrames, (4) Dask arrays, mentioning 
            linealg. } Additionally, BigData frameworks are better 
            suited for the 
            processing of large images. Scientific workflows are generally 
            file-based with neuroimaging frameworks only providing parallelism
            on the file front. Sparks main abstraction, the Resilient 
            Distributed Dataset (RDD), permits users to process large datasets.
            A library based on the RDD, \textit{linealg}, allows users to create
            vectors and matrices and use common Python libraries, such as 
            Numpy and Scikit-learn to manipulate the data. Dask similarly 
            provides the Dask bag for file manipulation, and the Dask array for
            the manipulation of large matrices. Dask provides its own API for
            the processing of large matrices that is based on Numpy. 
            Both frameworks also provide data structures to manipulate tabular
            data: the Spark Dataframe and the Dask Dataframe. Unlike Spark, the
            Dask Dataframe is based off of the Pandas Dataframe API.


        \section{Tool interfaces}\label{mod}
            \tristan{Since this section is about interfaces, I would
            rather call it ``Interfaces" or ``Tool interfaces".}
            Often times, users will want to integrate existing tools, in 
            addition to custom scripts, into their workflows. As workflow 
            engines, with the exception of Make, are typically not written in 
            Shell, it is necessary to create interfaces to these tools. These
            interfaces may range from simple calls to the command-line, to more 
            complex ones that track inputs, outputs and other parameters; ensure
            that all parameters are specified in the correct order and that the
            parameter and file types are all correct; capture all of the 
            standard input/output; and create folders to encapsulate task inputs
            and outputs. Workflow engines typically rely on more complex types 
            of interfaces in order to ensure correct functioning of the pipeline
            . As these interfaces can be rather complex, it is necessary for the 
            workflow engine to provide an intuitive way for users to create 
            interfaces for their custom tools.

            Early neuroimaging workflow engines, such as SPM, are monolithic, 
            and therefore, do not provide a mechanism for users to incorporate
            their custom tools \tristan{Users can still integrate tools as Matlab functions
            doing system calls}. Workflow engines, such as AA, were thus 
            conceived to enable engines such as SPM to be more modular. AA 
            allows users to define their tasks through XML specification. AA's 
            XML tasklist is simply an ordered list of tasks which also includes
            details on analysis-specific parameters. Dependencies are
            inferred by the program through analysis of the module interfaces.
            Module interfaces consist of MATLAB user script and an XML 
            descriptor. The descriptor contains details on the input and output
            data stream types (e.g. epi, structural, dicom\_header) 
            and the desired domain (e.g. study, subject, session). During 
            execution, each task is given its own unique folder to operate on. 
            The engine will copy data to and extract data from this folder.

            LONI, FastR, Taverna, Pegasus and Galaxy are other examples of 
            workflow engines that 
            use an XML interface to describe CLIs. LONI enables generation of 
            the XML-based module interface through its GUI. The module interface
            consists of details on \note{from website} module metadata (e.g.
            authors, package, version, name) and citation information, 
            command-line syntax, parameter and file types, dependencies and 
            output and error handling.

            FastR allows for the definition of workflow modules in both XML and 
            JSON. Module definitions are broken down into two components, 
            \textit{Tool} and \textit{Interface} class. The \textit{Tool} is the
            component that is written in either XML or JSON. It defines three
            components: general metadata, target and interface. General metadata
            consists of details such as name, version and authors. The target, 
            on the other hand, consists of details on the execution environment
            (e.g. operating system, interpreter and binary file). The last 
            component of the \textit{Tool}, the interpreter, provides details on
            the inputs and outputs of the tool (e.g. name, datatype, cardinality
            , location). FastR provides to schemas to validate and convert the 
            \textit{Tool} to a Python object
            : the \textit{Tool} schema and \textit{FastRInterface}. Should these
            schemas not meet the requirements of the user, the user may choose 
            to define their own schema.

            Tool addition in Taverna is primarily performed through the user 
            interface, but can also achieved through the creation of an XML
            descriptor. Taverna differs from other workflow engines in that it
            execute generic-style services (i.e. WSDL and REST) in addition to
            other types of Web Services, as well a external CLIs, R scripts and
            Java services. To interface with a CLI, the XML file must contain 
            the command to execute, file inputs and outputs (if required), if 
            standard input, output and error streams must be conserved and the 
            location where the command should execute\note{from website}.

            Pegasus is another workflow engine that uses an XML format to 
            define the CLI tools interface. The XML interfaces are typically 
            incorporated directly into the workflow DAX definition. However, a 
            separate multiline text-based 'Transformation Catalogue' file can
            be provided. Information encoded within the interfaces include
            tool identifier (typically namespace::version), the URL or filepath
            to the executable, site identifier of the executable's host, 
            executable type (e.g. installed or staged) and the profile. Pegasus
            can also execute containerized tools \tristan{how about the other ones?}. For this, a container 
            identifier, image URL, site identifier of the image, and container
            profiles must also be defined \note{from website}.

            While Galaxy is primarily a GUI-based application, an XML tool
            configuration file is required to add the tool to the application.
            The tool wrappers schema is very rich allowing for many tool 
            features to be defined. Similarly to many other workflow engine tool
            wrapper, the configuration file contains details on the tool name, 
            type, id, versions, command and input and output specifications. It
            also enables the execution of containerized tools. Furthermore,
            Galaxy enables users to specific actions to be performed on the 
            output files once created and define test cases.

            While many worfklow engines use XML or JSON to describe their tools,
            others wrap their tools in an interface defined by a
            programming/scripting language. Nipype, Pydpiper and Toil all use
            Python to create their wrappers; PSOM wrappers are written in
            MATLAB/Octave; OpenMOLE uses Java/Scala; and Make interfaces are
            written in Shell Script.

            Nipype's wrapper, written in Python, can encapsulate CLIs, SPM, 
            MATLAB and Python scripts. Attributes and methods that can be 
            defined for all interfaces include InputSpec, OutputSpec and a 
            Python dictionary of all expected outputs. InputSpec and OutputSpec
            are two Nipype classes. The former defines the input parameters of 
            a command and the second defines the outputs that are possibly 
            generated by the command. For CLIs, it is necessary to define the
            command. Additionally, Nipype will start X virtual framebuffer 
            before executing a CLI, if specified \tristan{you need to explain why}. For SPM interfaces, the job 
            type and name must be specified. Interfaces to MATLAB require 
            definition of the run function, while Python interfaces require
            the definition of the \textit{\_run\_interface} function. \note{from
            website}.

            Pydpiper's tools can either be custom Python code or CLIs. The 
            \textit{PipelineStage} and \textit{CmdStage} are used to 
            encapsulate the tools. The \textit{CmdStage} class is specifically 
            used to interface with CLIs, and thus, accepts a parsed array of the
            command and its parameters as input. Atoms, typically used to wrap
            around MINC file-format related tools, inherit from the
            \textit{CmdStage} class, and require the input MINC file (string or
            file handler) to be provided. A output file can also be specifiec, 
            but it must be of the same format as the MINC file (string or file 
            handler). Additionally, it is possible to pass multiple optional 
            arguments to the Atom.


            Toil is another workflow engine that describes their tool interfaces
            in Python. The wrappers, also known as \textit{Jobs} enable users to
            interface with custom Python code or Dockerized \note{cite Docker} 
            CLIs. Interfaces can be constructed either by creating a class that
            inherits from the \textit{Job} class or through the use of a 
            built-in function that converts the user-defined function (UDF) to a 
            Toil \textit{Job}. To convert, all that is required is the UDF,
            however, the user may also choose to include details on memory, 
            number of cores and disk space required. For Dockerized tools, Toil
            provides a built-in function that accepts the tool name, working dir
            and parameters of the tool.

            PSOM's tool interfaces are known as \textit{Bricks}. \textit{Bricks}
            can interface with any MATLAB function or CLI. Bricks require the
            definition of input and output files, in addition to a structure of
            program options. A useful feature found in PSOM is the ability 
            to perform a ``dry-run" of the Brick without executing the command.

            OpenMOLE has the ability to encapsulate UDF Scala scripts and CLI. 
            In the case of CLI tasks, it is also possible to containerize or 
            encapsulate with CARE \note{cite} to make it portable. OpenMOLE
            defines several classes to allow for the execution of different 
            types of tools. Like other framworks, it is vital to describe the
            inputs and outputs of the Tasks such that the execution of tasks 
            can be performed. 

            The Make pipeline is expressed in a makefile containing a set of 
            rules (Tool interfaces). Rules consist of three components: targets, 
            dependencies and recipes. The target is the output (typically a 
            file) of the 
            execution of a recipe and the dependencies are the input required to 
            produce the given target. Recipes are the shell commands that will
            generate the target from its dependencies. 

            Unlike scientific workflow engines, BigData frameworks do not 
            provide default interfaces for the execution of tools written in 
            languages different from their own. Users defining their workflows
            in Dask or the Spark and MapReduce using the Python API integrate
            Python UDFs in their workflow. While Dask users can reference their 
            UDF of Python function directly in their workflow, Spark users
            must define a lambda function within an action or transformation 
            that executes their UDF or library on the data. Similarly to Spark,
            functions referenced in MapReduce must be called from within a Map
            or Reduce task. MapReduce can additionally reference Java 
            code and Spark can call on code also written in Scala, Java and R.
            \note{verify if it's possible to mix-and-match different libraries
            ...don't think so} 

            For all workflow engines that do not, by default, interface with 
            command-line tools or containers, there are libraries that can be
            easily integrated to achieve that. Boutiques, for instance, is a 
            Python library that can execute command-line or containerized tools
            defined within a JSON schema. Furthermore, the library's richness
            compete with Galaxy's XML. Python-based BigData frameworks can 
            easily call on this library from within the workflow in order to
            communicate with the tools.

            \subsection{input and output definitions}
                TBD

            \tristan{Good material in here, but it lacks a bit structure.
            I like your structure of the previous sub-section, by language. I think
            here you could have two groups: (1) DSL descriptions (XML, JSON, etc) and (2)
            Python or Matlab descriptions. One advantage of DSLs is that they can 
            be more easily reused in different engines, while programming languages
            are more flexible, for instance Nipype allows outputs to be specified
            at runtime (dynamic number of inputs with names only known after
            the interface ran.)}


        \section{Workflow sharing}\label{sharing}
            
            
    \chapter{Performance}\label{performance}
        Neuroimaging frameworks are notoriously known to be compute-intensive.
        The tools and their respective workflows needed to incorporate 
        parallelism in order to save on computing time. Maintaining good 
        performance has become especially critical now that BigData has been
        introduced into the mix. The dichotomy remains between scientific 
        workflows and BigData workflows. While scientific workflows have made
        advances towards more performant engines, they still lack the 
        strategies BigData frameworks employ to reduce data transfers. 
        
        Another notable aspect to consider is that both types of engines 
        (scientific and BigData) are tightly coupled to the infrastructure
        they were intended to be used on. That is, as scientific workflow 
        try to remain efficient in their handling of compute resources, as such
        resources were previously scarce. In contrast, BigData frameworks 
        make efficient use of commodity clusters, where data transfers are 
        burdensome, particularly when faced with BigData.

        As some researchers are trying to shift towards the use of commodity 
        infrastructure while others are trying to process BigData on HPC, the 
        ideal workflow engine must run efficiently regardless of 
        infrastructure available. Otherwise, researchers are either left with
        workflows running inefficiently on resources or having to learn 
        multiple frameworks and rewrite their workflows depending on the 
        infrastructure selected.
 
        In this Chapter, we will compare scientific
        workflows to BigData frameworks on the basis of performance. 
        Section~\ref{fs} will look at what filesystems are required by the 
        engines. This is important because data movement of any kind is costly,
        and may be worse depending on the filesystems available. 
        Section~\ref{sched} will provide an overview of the schedulers 
        available and their compatibility with the workflow engines. 
        Section~\ref{other} will discuss other performance improvement 
        strategies employed by the workflow engines.
 
        \section{Filesystems}\label{fs}
            
            
            Unlike MapReduce and Spark-based workflows, Dask and scientific 
            workflows are not data-centric. They focus on the efficient 
            scheduling of tasks and rely on I/O and network transfers to
            communicate the data between nodes. With BigData workflows, such
            operations could incur significant performance penalties. 
            Filesystems. To mitigate this and remain efficient in the face of
            BigData, Dask implements lazy evaluation and in-memory processing.
            BigData platforms such as MapReduce rely heavily on I/O, but the 
            I/O performed occurs on the compute node rather than on 
            shared storaged, making it generally more efficient than scientific
            workflows. Additionally, MapReduce uses the concept of 
            locality to ensure limit data transfers across the network. Spark
            uses data locality, lazy evaluation and in-memory computing to 
            ensure its optimal performance.
            For the efficient processing of neuroimaging BigData, it will be
            necessary for scientific worflow engines to incorporate these 
            strategies.
        

            \subsection{Strategies}
                \subsubsection{in-memory computing}
                    Random Access Memory (RAM) is the fastest memory available
                    to workflow engines. BigData frameworks, such as Spark and 
                    Dask, leverage this feature by maintaining data in memory 
                    between tasks rather that saving them to local disk or 
                    shared memory which may incur significant overhead.

                    Spark and Dask can both read from filesystems available to
                    all nodes. When an action (Spark) or a compute (Dask) is 
                    called, the data is read from the storage and uploaded into
                    memory. Tasks use the data located in memory to compute the
                    results. Should data not fit in memory, the results may 
                    spill to local disk, in the instance where nodes contain
                    storage. Otherwise, the engines will spill data to shared 
                    filesystem. The later is a common case in HPC systems as 
                    storage and compute resources are typically separate, 
                    however, these shared filesystems are typically slower than 
                    local storage, and therefore, there will be additional 
                    performance penalties.

                    As data is not stored within a resilient distributed 
                    filesystem such as Lustre and HDFS, it risks being 
                    permanently lost if a node failure occurs. Spark and Dask
                    have both addressed this through the notion of 
                    \textit{lineage}. Lineage refers to the set of operations
                    that the data has undergone. By recording the lineage, lost
                    data will always be recoverable as the set of operations
                    to reproduce the data are known. Should a node be lost to 
                    the cluster, Spark and Dask reschedule the tasks required 
                    to rebuild the dataset to other nodes.

                    \note{mention caching}

                    
                    While Spark and Dask both rely on the concept of shared 
                    memory, Spark avoids data transfers as often as possible by
                    scheduling tasks to nodes where the memory is located. Dask
                    , however, will schedule its tasks to the next available 
                    node.
                    
                    
                \subsubsection{Data locality}
                    Data locality, with respect to BigData frameworks, is a
                    scheduling
                    strategy in which location of the data is known by the 
                    scheduler and tasks are scheduled to the location where the
                    necessary data is located. It was one of the first main 
                    strategies implemented to make BigData processing more 
                    efficient. The reasoning behind data locality is that 
                    network transfers are expensive, particularly within 
                    commodity infrastracture, and therefore transferring large 
                    volumes of data across the network for every task would be 
                    rather expensive.

                    While MapReduce and Spark both partition data across the 
                    network similarly, they achieve data locality slightly 
                    differently. MapReduce can use the local filesystem in 
                    standalone mode, but requires use of a distributed 
                    filesystem executing on the compute nodes such as the 
                    Hadoop Distributed File System (HDFS). The reason as to why
                    parallel file systems such as Lustre do not work is that 
                    the storage nodes on such file systems are separate from 
                    the compute nodes. Separate storage and compute nodes 
                    eliminate any possiblity of data locality as network 
                    transfers will occur regardless of storage node. When the 
                    next MapReduce task is available, the master node will 
                    query the distributed file system to determine the 
                    available nodes closest. The master will first attempt to
                    schedule the task on a node containing the data, but if 
                    that node is not available, it will attempt to schedule
                    the task on the same rack. If all else fails, the task will
                    be scheduled on a node found on a different rack. 

                    In contrast to MapReduce, Spark does not require data to 
                    be stored on a HDFS-like filesystem. It can read data from
                    any form of shared-filesystem, including Lustre. If data 
                    is extracted from a non HDFS-like filesystem, the initial 
                    loading of data will not use data locality. However, in all
                    future tasks in which the data is already loaded into an 
                    RDD, the scheduler will be able to assign task to the most
                    favorable partition dictated by delay scheduling.

                    
                \subsubsection{lazy evaluation}

                    Lazy evaluation is a strategy used throughout computer 
                    science to improve performance of operations. Rather than
                    evaluating a expression immediately, it will wait until it 
                    is required in order to compute the result. This is 
                    achieved in Spark through the calling of an \textit{action}
                    on an RDD and in Dask through the calling of a 
                    \textit{compute} function on one of its data structures.
                    By using lazy evaluation, the engines are capable of 
                    constructing graphs that result in the most optimal
                    task order of execution.

            \subsection{BigData Filesystem optimizations for HPC}

                Writting BigData to Lustre is very costly compared to local 
                node storage. BigData filesystems like HDFS may not necessarily
                be the best solution for HPC as they will not only use up 
                storage on compute nodes, but also replicate the data across 
                the cluster taking up a significant amount of storage space.
                Moreover, while BigData frameworks typically execute on a 
                dedicated BigData cluster, they will be on a shared cluster in
                HPC, with many processed trying to access the memory. The 
                following are two optimizations made for  \note{two or one}

                Alluxio, formerly known as Tachyon, is an in-memory filesystem
                that stores lineage data to offset the cost of writing data to 
                disk. The assumption that Alluxio makes is that it will be less
                costly to recompute the data that to store the data to disk and
                replicate across the network for fault tolerance. However,
                unlike Spark applications, Alluxio is meant to run indefinitely
                , making it unfavourable, in some cases, to recompute the data
                from its starting point. For this, Alluxio evaluates the 
                lineage data and selects the 

                Triple-H leverages the heterogeneity of HPC clusters to 
                optimize the data's placement across the cluster for most 
                efficient use. It has two modes, default and Lustre integrated,
                that allows it to execute on systems with and without access
                to a distributed filesystem. 
            
            \note{scientific: shared FS such as Lustre, NFS, S3
            big data: mapreduce - HDFS, spark and dask shared or distributed}
        \section{BigData Scheduling}\label{sched}

            
            Scientific workflows typically rely on HPC schedulers for 
            distributed scheduling on clusters. These schedulers include SLURM,
            PBS,TORQUE, SGE, HTCondor and Mesos. Additionally, they may provide
            their own form of scheduling for computations on a single node. 
            Nipype uses mutliproc... etc. BigData frameworks, such as Dask,
            also provide interfaces to these HPC schedulers. However, other
            frameworks such as MapReduce and Spark are limited to BigData 
            clusters such as Yet Another Resource Negotiator (YARN and MESOS).
            Spark additionally provides its own standalone scheduler. Similarly
            to scientific workflows, all three BigData frameworks provide their
            own schedulers for single-node scheduling.

            Spark's standalone scheduler is a simple scheduler that can be used
            in distributed workflows. It can execute in two modes: client and
            cluster. In client mode, the  \note{cannot run in cluster mode in pyspark} 

            YARN needs hadoop cluster

            Mesos can run on HPC

            dask distributed
            \note{scientific: SLURM, PBS, SGE, HTCondor, Mesos (sometimes)
            big data: mapreduce - local or YARN, Spark - standalone, YARN or 
            Mesos, dask: any (easy to extend to any scheduler)}
        \section{Performance enhancement strategies}\label{other}
            scientific: workflow checkpointing (looking at file hashes)
            big data: data locality, in memory computing, lazy evaluation
    \chapter{Portability}\label{portability}
        In order for a workflow engine to be utilized, it much be simple for 
        users to install and execute the data. Platform-independence of both 
        the frameworks and tools, as well as, simple installation of all 
        necessary components and straightforward execution of workflows  are a 
        requirement for frameworks and their respective pipelines to be 
        portable. Without portability, users are limited to executing certain
        frameworks only or dealing with the significant overhead of making 
        these engines work on the system available to them. In this chapter, 
        we evaluate the portability of workflows on the basis of the following
        three components: 1) platform-independence, 2) tool installation and
        3) execution setup. We also examine containerization, one of the 
        strategies used to improve portability.

        \section{Platform-independence}
            Achieving platform-independence allows users to use the workflow 
            engine in any available environment. This however does not imply 
            that the tools, and as a consequence, the pipelines created will
            also be platform-independent. Many workflow engines have opted to 
            be platform independent. These workflow engines include: Nipype,
            PSOM, SPM, FASTR, LONI, AA, Pydpiper, Taverna, MapReduce, Spark and
            Dask. These workflow engines achieve platform independence through
            running on the Java Virtual Machine (JVM) or being written in 
            Python. Platform-independent engines that are Java-based include: 
            PSOM, SPM, LONI, AA, Taverna, MapReduce and Spark. The remainder of
            the pipelines are implemented in Python. The remainder of the 
            workflow engines (Pegasus, Make, OpenMOLE, Nextflow, Galaxy, and 
            Toil are not platform-independent, but can execute on UNIX-like 
            systems.

            Engines built ontop of a platform-independent language does not 
            necessarily imply platform independence. For instance, OpenMOLE is 
            built in Scala and Netflow runs on Groovy, yet neither are platform
            independent. In the case of OpenMOLE, this is due to its CARETask
            blackbox, which is only possible for Linux environments. However,
            many tools and programs available for incorporation into workflows
            are platform-dependent. For example, even though Nipype is 
            platform-independent, common tools used in Nipype modules, such as 
            FreeSurfer and FSL, are Linux-based. Moreover, HPC infrastructure 
            is usually Linux-based and it is therefore crucial that all 
            workflows are compatible with and tested on Linux systems.
             
       \section{Tool installation}

            Unless building a workflow exclusively based on custom
            functions or using a monolithic workflow engine such as SPM, users
            will need to install the tools necessary for their workflow on
            their computer or cluster. Depending on the workflow engine 
            selected, this may be managed by the engine or performed manually 
            by the user. Most install the dependencies themselves. Workflow engines that
            delegate tool installation to the user include: Nipype, PSOMi,  

        Many of these platforms have selected to be written in 
        platform-independent languages. However,  Due to the modularity of these
        frameworks, it may be necessary
        java or python backend (most) typically platform independent (some 
        exceptions)
        all tools used in the pipeline must be installed on system
        Taverna - exception as relies on web services
        \section{Containers}

             Containeri
            use of containers increase portability. none of the scientific 
            pipelines can be containerized and communicate with the scheduler 
            outside of the container
            big data frameworks can
    \chapter{Reproducibility}\label{reproducibility}
        \section{Provenance Tracking}
        big data frameworks: limited to lineage
        \section{containerization}
    \chapter{Discussion/Conclusion}


    \addcontentsline{toc}
        {chapter}{Bibliography} 
        \bibliography{bibliography}
        \bibliographystyle{ieeetr}
\end{document}
